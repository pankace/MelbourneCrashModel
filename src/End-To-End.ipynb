{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define data paths, reload config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import yaml\n",
    "import os\n",
    "import subprocess\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import sys\n",
    "import argparse\n",
    "import osmnx as ox\n",
    "import fiona\n",
    "import shutil\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import geojson\n",
    "import json\n",
    "import requests\n",
    "import yaml\n",
    "import sys\n",
    "from shapely.geometry import mapping, Polygon, LineString, LinearRing\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"C:\\\\Users\\\\Daniel\\\\Documents\\\\ML\\\\Transurban V2\"\n",
    "CURR_FP = os.path.join(BASE_DIR, 'src')\n",
    "CONFIG_FP = os.path.join(BASE_DIR, 'src', 'config', 'Melbourne.yml')\n",
    "config_file = CONFIG_FP\n",
    "data_dir = os.path.join(BASE_DIR, 'data', 'Melbourne')\n",
    "DATA_DIR = data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths and check directories exist\n",
    "RAW_DIR = os.path.join(data_dir, \"raw\")\n",
    "RAW_CRASH_DIR = os.path.join(RAW_DIR, 'crash')\n",
    "PROCESSED_CRASH_DIR = os.path.join(data_dir, 'processed', 'crash')\n",
    "PROCESSED_MAPPING_DIR = os.path.join(data_dir, 'processed', 'mapping')\n",
    "MAP_DIR = os.path.join(DATA_DIR, 'processed/maps')\n",
    "DOC_DIR = os.path.join(DATA_DIR, 'processed/mapping')\n",
    "PROCESSED_CRASH_DIR = os.path.join(DATA_DIR, 'processed/crash')\n",
    "PROCESSED_DATA_DIR = os.path.join(DATA_DIR, 'processed')\n",
    "CRASH_DIR = os.path.join(BASE_DIR, 'data/Melbourne/processed/crash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CONFIG_FP) as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FP = os.path.join(BASE_DIR, 'data', config['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import tzlocal\n",
    "from data.util import geocode_address\n",
    "\n",
    "BASE_DIR = \"C:\\\\Users\\\\Daniel\\\\Documents\\\\ML\\\\Transurban V2\"\n",
    "\n",
    "\n",
    "def make_config_file(yml_file, timezone, city, folder, crash_file_path, map_file_path, map_inters_file_path, atmosphere_file_path, merged_file_path, cat_feat, cont_feat, keep_feat):\n",
    "\n",
    "    address = geocode_address(city)\n",
    "\n",
    "    f = open(yml_file, 'w')\n",
    "    f.write(\n",
    "        \"# City name\\n\" +\n",
    "        \"city: {}\\n\".format(city) +\n",
    "        \"# City centerpoint latitude & longitude\\n\" +\n",
    "        \"city_latitude: {}\\n\".format(address[1]) +\n",
    "        \"city_longitude: {}\\n\".format(address[2]) +\n",
    "        \"# City's time zone [defaults to local to,e of computer]\\n\" +\n",
    "        \"timezone: {}\\n\".format(timezone) +\n",
    "        \"# The folder under data where this city's data is stored\\n\" +\n",
    "        \"name: {}\\n\".format(folder) +\n",
    "        \"# Limit crashes to between start and end date\\n\" +\n",
    "        \"startdate: \\n\" +\n",
    "        \"enddate: \\n\" +\n",
    "        \"#################################################################\\n\" +\n",
    "        \"crash_files:\\n\" +\n",
    "        \"  {}\\n\".format(crash_file_path) +\n",
    "        \"  {}\\n\".format(map_file_path) +\n",
    "        \"  {}\\n\".format(map_inters_file_path) +\n",
    "        \"  {}\\n\".format(atmosphere_file_path) +\n",
    "        \"cat_feat: {} \\n\".format(cat_feat) +\n",
    "        \"cont_feat: {} \\n\".format(cont_feat) +\n",
    "        \"keep_feat: {} \\n\".format(keep_feat) +\n",
    "        \"merged_data: {}\".format(merged_file_path)\n",
    "    )\n",
    "\n",
    "    f.close()\n",
    "    print(\"Wrote new configuration file in {}\".format(yml_file))\n",
    "\n",
    "\n",
    "def make_js_config(jsfile, city, folder):\n",
    "    address = geocode_address(city)\n",
    "\n",
    "    f = open(jsfile, 'w')\n",
    "    f.write(\n",
    "        'var config = {\\n' +\n",
    "        '    MAPBOX_TOKEN: \"pk.eyJ1IjoiZGVsZXdpczEzIiwiYSI6ImNqb3BjaTYzaDAwdjQzcWxsa3hsNzFtbmYifQ.yKj5c8ODg6yN0xTwmYS1LQ\",\\n' +\n",
    "        '    cities: [\\n' +\n",
    "        '        {\\n' +\n",
    "        '            name: \"{}\",\\n'.format(city) +\n",
    "        '            id: \"{}\",\\n'.format(folder) +\n",
    "        '            latitude: {},\\n'.format(str(address[1])) +\n",
    "        '            longitude: {},\\n'.format(str(address[2])) +\n",
    "        '        }\\n' +\n",
    "        '    ]\\n' +\n",
    "        '}\\n'\n",
    "    )\n",
    "    f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = \"Melbourne\"\n",
    "folder = \"Melbourne\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Using the default raw file locations. If they do not exist, please include raw file path arguments in cmd call.')\n",
    "crash_file = 'C:/Users/Daniel/Documents/ML/Transurban V2/data/raw/crash.csv'\n",
    "map_file = 'C:/Users/Daniel/Documents/ML/Transurban V2/data/raw/map.csv'\n",
    "map_inters_file = 'C:/Users/Daniel/Documents/ML/Transurban V2/data/raw/map_inters.csv'\n",
    "atmosphere_file = 'C:/Users/Daniel/Documents/ML/Transurban V2/data/raw/atmosphere.csv'\n",
    "\n",
    "# Get our various file paths\n",
    "DATA_FP = os.path.join(BASE_DIR, 'data', folder)\n",
    "PROCESSED_DIR = os.path.join(DATA_FP, 'processed')\n",
    "RAW_DIR = os.path.join(DATA_FP, 'raw')\n",
    "RAW_CRASH_DIR = os.path.join(RAW_DIR, 'crash')\n",
    "\n",
    "crash_file_path = os.path.join(RAW_CRASH_DIR, 'crash.csv')\n",
    "map_file_path = os.path.join(RAW_CRASH_DIR, 'map.csv')\n",
    "map_inters_file_path = os.path.join(RAW_CRASH_DIR, 'map_inters.csv')\n",
    "atmosphere_file_path = os.path.join(RAW_CRASH_DIR, 'atmosphere.csv')\n",
    "merged_file_path = os.path.join(PROCESSED_DIR, 'canon.csv.gz')\n",
    "\n",
    "# Define our categorical / continuous features for usage in modelling\n",
    "cat_feat = ['HOUR', 'DAY_OF_WEEK', 'MONTH', 'DEGREE_URBAN', 'LIGHT_COND', 'ATMOSPH_COND', 'NODE_TYPE_INT',\n",
    "            'COMPLEX_INT', 'hwy_type', 'inter', 'intersection_segments', 'lanes', 'oneway', 'signal', 'streets', 'direction']\n",
    "\n",
    "cont_feat = ['SPEED_ZONE', 'osm_speed', 'LAST_7_DAYS', 'LAST_30_DAYS', 'LAST_365_DAYS', 'LAST_1825_DAYS', 'LAST_3650_DAYS']\n",
    "\n",
    "# Define our features to keep until the last step where we strip down to modelling features\n",
    "keep_feat = cat_feat + cont_feat + ['display_name', 'intersection', 'segment_id']\n",
    "\n",
    "# Create our data paths\n",
    "if not os.path.exists(DATA_FP):\n",
    "    print(\"Making directory structure under \" + DATA_FP)\n",
    "    os.makedirs(DATA_FP)\n",
    "    os.makedirs(os.path.join(DATA_FP, 'raw'))\n",
    "    os.makedirs(os.path.join(DATA_FP, 'processed'))\n",
    "    os.makedirs(os.path.join(DATA_FP, 'standardized'))\n",
    "    os.makedirs(RAW_CRASH_DIR)\n",
    "\n",
    "    # We copy across all raw data files.\n",
    "    # Note: Merged is not copied because not yet created.\n",
    "    shutil.copyfile(crash_file, crash_file_path)\n",
    "    shutil.copyfile(map_file, map_file_path)\n",
    "    shutil.copyfile(map_inters_file, map_inters_file_path)\n",
    "    shutil.copyfile(atmosphere_file, atmosphere_file_path)\n",
    "else:\n",
    "    print(folder + \"folder already initialized, skipping\")\n",
    "\n",
    "# Create our yml config file\n",
    "yml_file = os.path.join(BASE_DIR, 'src/config/' + folder + '.yml')\n",
    "if not os.path.exists(yml_file):\n",
    "    make_config_file(yml_file, tzlocal.get_localzone().zone, city, folder, crash_file_path, map_file_path, map_inters_file_path, atmosphere_file_path, merged_file_path, cat_feat, cont_feat, keep_feat)\n",
    "\n",
    "# Create our js config file\n",
    "reports_file_path = os.path.join(BASE_DIR, 'reports', folder)\n",
    "print(reports_file_path)\n",
    "if not os.path.exists(reports_file_path):\n",
    "    print('Making reports file path')\n",
    "    os.makedirs(reports_file_path)\n",
    "\n",
    "js_file_path = os.path.join(BASE_DIR, 'reports', folder, 'config.js')\n",
    "if not os.path.exists(js_file_path):\n",
    "    print(\"Writing config.js\")\n",
    "    make_js_config(js_file_path, city, folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize Crashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_clean_combine_crash(RAW_CRASH_DIR):\n",
    "    # Get file names and read in csv's\n",
    "    crash_file = os.path.join(RAW_CRASH_DIR, 'crash.csv')\n",
    "    map_file = os.path.join(RAW_CRASH_DIR, 'map.csv')\n",
    "    map_inters_file = os.path.join(RAW_CRASH_DIR, 'map_inters.csv')\n",
    "    atmosphere_file = os.path.join(RAW_CRASH_DIR, 'atmosphere.csv')\n",
    "\n",
    "    crash_df = pd.read_csv(crash_file)\n",
    "    map_df = pd.read_csv(map_file)\n",
    "    map_inters_df = pd.read_csv(map_inters_file)\n",
    "    atmosphere_df = pd.read_csv(atmosphere_file)\n",
    "\n",
    "    # crash_df: drop unwanted columns and establish mappings\n",
    "    crash_df_cols_reduced = ['ACCIDENT_NO', 'ACCIDENTDATE', 'ACCIDENTTIME', 'DAY_OF_WEEK', 'LIGHT_CONDITION', 'NODE_ID', 'ROAD_GEOMETRY', 'SPEED_ZONE']\n",
    "    geom_mapping_cols = ['ROAD_GEOMETRY', 'Road Geometry Desc']\n",
    "    accident_type_mapping_cols = ['ACCIDENT_TYPE', 'Accident Type Desc']\n",
    "    DCA_code_mapping_cols = ['DCA_CODE', 'DCA Description']\n",
    "    light_condition_mapping_cols = ['LIGHT_CONDITION', 'Light Condition Desc']\n",
    "\n",
    "    geom_mapping = crash_df[geom_mapping_cols]\n",
    "    geom_mapping = geom_mapping.drop_duplicates().sort_values(by='ROAD_GEOMETRY').reset_index(drop=True)\n",
    "\n",
    "    accident_type_mapping = crash_df[accident_type_mapping_cols]\n",
    "    accident_type_mapping = accident_type_mapping.drop_duplicates().sort_values(by='ACCIDENT_TYPE').reset_index(drop=True)\n",
    "\n",
    "    light_condition_mapping = crash_df[light_condition_mapping_cols]\n",
    "    light_condition_mapping = light_condition_mapping.drop_duplicates().sort_values(by='LIGHT_CONDITION').reset_index(drop=True)\n",
    "\n",
    "    DCA_code_mapping = crash_df[DCA_code_mapping_cols]\n",
    "    DCA_code_mapping = DCA_code_mapping.drop_duplicates().sort_values(by=\"DCA_CODE\").reset_index(drop=True)\n",
    "\n",
    "    crash_df_reduced = crash_df[crash_df_cols_reduced]\n",
    "\n",
    "    # Map_df: drop unwanted columns, create node type mapping\n",
    "    node_type_mapping = pd.DataFrame({'NODE_TYPE_INT': [0, 1, 2, 3], 'NODE_TYPE': ['I', 'N', 'O', 'U'], 'NODE_DESC': ['Intersection', 'Non-intersection', 'Off-road', 'Unknown']})\n",
    "\n",
    "    map_df['NODE_TYPE_INT'] = \"\"\n",
    "    for index, row in node_type_mapping.iterrows():\n",
    "        map_df['NODE_TYPE_INT'].loc[map_df['NODE_TYPE'] == row['NODE_TYPE']] = row['NODE_TYPE_INT']\n",
    "\n",
    "    map_df_reduced_cols = ['ACCIDENT_NO', 'NODE_ID', 'NODE_TYPE_INT', 'LGA_NAME', 'Deg Urban Name', 'Lat', 'Long']\n",
    "    map_df_reduced = map_df[map_df_reduced_cols]\n",
    "\n",
    "    # map_iters_df: drop unwanted columns, creat node-to-complex node mapping\n",
    "    complex_node_mapping_cols = ['NODE_ID', 'COMPLEX_INT_NO']\n",
    "    complex_node_mapping = map_inters_df[complex_node_mapping_cols]\n",
    "    complex_node_mapping = complex_node_mapping.drop_duplicates().sort_values(by=\"NODE_ID\").reset_index(drop=True)\n",
    "\n",
    "    map_inters_df_reduced = map_inters_df[['ACCIDENT_NO', 'COMPLEX_INT_NO']]\n",
    "\n",
    "    # atmosphere_df: drop unwanted columns, create atmosphere mapping\n",
    "    atmosphere_mapping_cols = ['ATMOSPH_COND', 'Atmosph Cond Desc']\n",
    "    atmosphere_mapping = atmosphere_df[atmosphere_mapping_cols]\n",
    "    atmosphere_mapping = atmosphere_mapping.drop_duplicates().sort_values(by=\"ATMOSPH_COND\").reset_index(drop=True)\n",
    "\n",
    "    atmosphere_df_reduced_cols = ['ACCIDENT_NO', 'ATMOSPH_COND']\n",
    "    atmosphere_df_reduced = atmosphere_df[atmosphere_df_reduced_cols]\n",
    "\n",
    "    # Drop duplicates from all dataframes\n",
    "    # Note: most of these duplicates are legitimate\n",
    "    # Chain effects of a crash are given different incident numbers. We will treat it as one crash however.\n",
    "    crash_df_reduced.drop_duplicates(subset=\"ACCIDENT_NO\", inplace=True)\n",
    "    map_df_reduced.drop_duplicates(subset=\"ACCIDENT_NO\", inplace=True)\n",
    "    map_inters_df_reduced.drop_duplicates(subset=\"ACCIDENT_NO\", inplace=True)\n",
    "    atmosphere_df_reduced.drop_duplicates(subset=\"ACCIDENT_NO\", inplace=True)\n",
    "\n",
    "    # Begin joining dataframes on 'ACCIDENT_NO'.\n",
    "    # Joining by 'outer', means that if some accident numbers are in one DF but not in another, the accident will still be recorded but will be missing columns\n",
    "    # The validate option ensures that when merging, each DF only has one instance of each accident number\n",
    "    crashes_and_atmos = pd.merge(crash_df_reduced, atmosphere_df_reduced, on='ACCIDENT_NO', how='outer', validate='one_to_one')\n",
    "    inters_and_complex = pd.merge(map_df_reduced, map_inters_df_reduced, on='ACCIDENT_NO', how='outer', validate='one_to_one')\n",
    "    crashes_df = pd.merge(crashes_and_atmos, inters_and_complex, on='ACCIDENT_NO', how='outer', validate='one_to_one')\n",
    "\n",
    "    # Many NA's within the 'COMPLEX_INT_NO' column. Fill some of these.\n",
    "    # Some NA's associated with strange NODE_ID's from original crash DF, meaning we couldn't map to Lat / Lon. Remove these.\n",
    "    # Drop the extra 'NODE_ID' column we have gained\n",
    "    crashes_df['COMPLEX_INT_NO'] = crashes_df['COMPLEX_INT_NO'].fillna(0)\n",
    "    crashes_df.dropna(subset=['Lat', 'Long'], inplace=True)\n",
    "    crashes_df.drop(['NODE_ID_y'], axis=1, inplace=True)\n",
    "\n",
    "    # Make sure there are no more NA values left anywhere within DF\n",
    "    if len(crashes_df[crashes_df.isna().any(axis=1)]) != 0:\n",
    "        print('There are still NA values left within crashes_df during standardization')\n",
    "        print('Please check this manually. Exiting.')\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Standardize column names\n",
    "    crashes_df.rename(columns={\"NODE_ID_x\": \"NODE_ID\", \"ACCIDENTDATE\": \"ACCIDENT_DATE\", \"ACCIDENTTIME\": \"ACCIDENT_TIME\",\n",
    "                               \"LGA_NAME\": \"SUBURB\", \"Deg Urban Name\": \"DEGREE_URBAN\", \"Lat\": \"LAT\", \"Long\": \"LON\", \"LIGHT_CONDITION\": \"LIGHT_COND\"}, inplace=True)\n",
    "\n",
    "    # Change dtype of columns to integers [these were changed due to the merge step creating NA values]\n",
    "    crashes_df[['NODE_ID', 'COMPLEX_INT_NO']] = crashes_df[['NODE_ID', 'COMPLEX_INT_NO']].astype(int)\n",
    "\n",
    "    # Feature engineering\n",
    "    # Add a binary value for if a crash occured at a complex node\n",
    "    # Add values for Hour / Month as seasonality features\n",
    "    crashes_df['COMPLEX_NODE'] = 0\n",
    "    crashes_df['COMPLEX_NODE'].loc[crashes_df['COMPLEX_INT_NO'] > 0] = 1\n",
    "    crashes_df['DATE_TIME'] = pd.to_datetime(crashes_df['ACCIDENT_DATE'] + \" \" + crashes_df['ACCIDENT_TIME'], format=\"%d/%m/%Y %H.%M.%S\")\n",
    "    crashes_df['HOUR'] = crashes_df.DATE_TIME.dt.hour\n",
    "    crashes_df['MONTH'] = crashes_df.DATE_TIME.dt.month\n",
    "\n",
    "    # Once again drop unwanted columns\n",
    "    crashes_df.drop(['ACCIDENT_DATE', 'ACCIDENT_TIME', 'COMPLEX_INT_NO'], axis=1, inplace=True)\n",
    "\n",
    "    # Reorder columns for easier reading\n",
    "    crashes_df = crashes_df[['ACCIDENT_NO', 'DATE_TIME', 'MONTH', 'HOUR', 'DAY_OF_WEEK', 'LAT', 'LON', 'SUBURB', 'NODE_ID', 'NODE_TYPE_INT', 'COMPLEX_NODE', 'LIGHT_COND', 'ATMOSPH_COND', 'SPEED_ZONE', 'ROAD_GEOMETRY', 'DEGREE_URBAN']]\n",
    "\n",
    "    # Set 'ACCIDENT_NO' to be the index\n",
    "    crashes_df.set_index('ACCIDENT_NO', inplace=True)\n",
    "\n",
    "    # Put various mappings into a tuple to allow for easier transport\n",
    "    mappings = (geom_mapping, accident_type_mapping, DCA_code_mapping, light_condition_mapping, node_type_mapping, atmosphere_mapping)\n",
    "\n",
    "    return crashes_df, mappings\n",
    "\n",
    "\n",
    "def output_crash_csv(PROCESSED_CRASH_DIR, PROCESSED_MAPPING_DIR, crashes_df, mappings):\n",
    "\n",
    "    geom_mapping, accident_type_mapping, DCA_code_mapping, light_condition_mapping, node_type_mapping, atmosphere_mapping = mappings\n",
    "\n",
    "    # Output resulting crashes_df and all mappings\n",
    "    if not os.path.exists(PROCESSED_CRASH_DIR):\n",
    "        os.makedirs(PROCESSED_CRASH_DIR)\n",
    "\n",
    "    if not os.path.exists(PROCESSED_MAPPING_DIR):\n",
    "        os.makedirs(PROCESSED_MAPPING_DIR)\n",
    "\n",
    "    crashes_path = os.path.join(PROCESSED_CRASH_DIR, 'crashes.csv')\n",
    "    crashes_df.to_csv(crashes_path)\n",
    "\n",
    "    mapping_dfs = [geom_mapping, accident_type_mapping, DCA_code_mapping,\n",
    "                   light_condition_mapping, node_type_mapping, atmosphere_mapping]\n",
    "    mapping_names = ['geom_mapping.csv', 'accident_type_mapping.csv', 'DCA_code_mapping.csv',\n",
    "                     'light_condition_mapping.csv', 'node_type_mapping.csv', 'atmosphere_mapping.csv']\n",
    "\n",
    "    for mapping_df, mapping_name in zip(mapping_dfs, mapping_names):\n",
    "        save_path = os.path.join(PROCESSED_MAPPING_DIR, mapping_name)\n",
    "        mapping_df.to_csv(save_path)\n",
    "\n",
    "\n",
    "def output_crash_json(PROCESSED_CRASH_DIR, crashes_df):\n",
    "    output_file = PROCESSED_CRASH_DIR + '/crashes.json'\n",
    "    crashes_df.to_json(output_file, orient='index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_df, mappings = read_clean_combine_crash(RAW_CRASH_DIR)\n",
    "output_crash_csv(PROCESSED_CRASH_DIR, PROCESSED_MAPPING_DIR, crashes_df, mappings)\n",
    "output_crash_json(PROCESSED_CRASH_DIR, crashes_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA GENERATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OSM create maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURR_DIR = 'C:\\\\Users\\\\Daniel\\\\Documents\\\\ML\\\\Transurban V2\\\\src\\\\data'\n",
    "sys.path.append(CURR_DIR)\n",
    "\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_osm_polygon(city):\n",
    "    \"\"\"Interrogate the OSM nominatim API for a city polygon.\n",
    "\n",
    "    Nominatim may not always return city matches in the most intuitive order,\n",
    "    so results need to be searched for a compatible polygon. The index of the\n",
    "    polygon is required for proper use of osmnx.graph_from_place(). Some cities\n",
    "    do not have a polygon at all, in which case they defer to using\n",
    "    graph_from_point() with city lat & lng.\n",
    "\n",
    "    Args:\n",
    "        city (str): city to search for\n",
    "    Returns:\n",
    "        int: index of polygon+1 (becomes the correct 'which_result' value)\n",
    "        None: if no polygon found\n",
    "    \"\"\"\n",
    "\n",
    "    # Old Request that I couldn't get working for greater melbourne region\n",
    "    # search_params = {'format': 'json', 'limit': 5, 'dedupe': 0, 'polygon_geojson': 1, 'city': city}\n",
    "    # url = 'https://nominatim.openstreetmap.org/search'\n",
    "    # response = requests.get(url, params=search_params)\n",
    "\n",
    "    # Returns the Greater Melbourne region.\n",
    "    # The response object can be dug into via enumerate.\n",
    "    # match['geojson'] looks like {'type' : 'Polygon', 'coordinates': [[[x, y], [x2, y2], ...]]}\n",
    "    #response = requests.get('https://nominatim.openstreetmap.org/search.php?q=greater+melbourne&polygon_geojson=1&format=json')\n",
    "    response = requests.get('https://nominatim.openstreetmap.org/search.php?q=melbourne&polygon_geojson=1&format=json')\n",
    "    for index, match in enumerate(response.json()):\n",
    "\n",
    "        # To be used by graph_from_place needs to be a Polygon or MultiPolygon\n",
    "        if (match['geojson']['type'] in ['Polygon', 'MultiPolygon']):\n",
    "            return index + 1, match['geojson']\n",
    "\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def expand_polygon(polygon, points_file, verbose=False, max_percent=.1, expand_polygon_bool=False):\n",
    "    \"\"\"\n",
    "    Read the crash data, determine what proportion of crashes fall outside\n",
    "    the city polygon\n",
    "    Args:\n",
    "        polygon - city polygon\n",
    "        points_file - json points file\n",
    "        Optional: max_percent (in case you want to override the maximum\n",
    "            percent that can be outside the original polygon to buffer)\n",
    "    Returns:\n",
    "        Updated polygon if it was a polygon to start with, None otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    # Only support for polygons\n",
    "    if polygon['type'] != 'Polygon':\n",
    "        if verbose:\n",
    "            print('Within expand_polygon. Polygon\\'s type is {}. We only support Polygon'.format(polygon['type']))\n",
    "        return None\n",
    "\n",
    "    # Remap polygon coordinates using get_reproject_point [default from 4326 to 3857 methods of geodesy]. Note 4326 is our normal concept of lat / lon.\n",
    "    # In general, 4326 is good for accurate storage, but is 3D. 3857 is good for 2D representation, but has issues with accuracy.\n",
    "    # There is some strangeness here with regards to ordering of x[1] and x[0]. This is because get_reproject_point flips it around, but didn't want to fix as might break something else.\n",
    "    polygon_coords = [util.get_reproject_point(x[1], x[0], verbose, coords=True) for x in polygon['coordinates'][0]]\n",
    "    if verbose:\n",
    "        print('Within osm_create_maps.expand_polygon')\n",
    "        print('Remapping lat / lon via utils.get_reproject_point from epsg:4326 to epsg:3857 by default')\n",
    "        print('Coords originally looked like:', polygon['coordinates'][0][0][1], polygon['coordinates'][0][0][0])\n",
    "        print('Transformed coords look like:', polygon_coords[0])\n",
    "\n",
    "    # Use shapely.geometry.polygon to create polygon object from coordinates\n",
    "    poly_shape = Polygon(polygon_coords)\n",
    "    if verbose:\n",
    "        print('Having obtained poly_coords we create polygon object which looks like:', poly_shape)\n",
    "\n",
    "    # Saving poly_shape out for visualisation\n",
    "    schema = {\n",
    "        'geometry': 'Polygon',\n",
    "        'properties': {'id': 'int'},\n",
    "    }\n",
    "\n",
    "    with fiona.open(os.path.join(MAP_DIR, \"polygon_coords.shp\"), \"w\", \"ESRI Shapefile\", schema) as output:\n",
    "        output.write({\n",
    "            'geometry': mapping(poly_shape),\n",
    "            'properties': {'id': 123}\n",
    "        })\n",
    "\n",
    "    # Reading in records from point file [in this case]. Projects lat and lon from 4326 to 3857.\n",
    "    records = util.read_records(points_file, 'crash', verbose=verbose)\n",
    "    if verbose:\n",
    "        print('Feeding into read_records:')\n",
    "        print('points_file:', points_file)\n",
    "        print('Having created our record objects here is an example:', records[0])\n",
    "\n",
    "    # Saving our records to a point shp file so we can visualise the geolocations of the crashes\n",
    "    # for record in records:\n",
    "\n",
    "    # If expand_polygon is true, check how many points from our crash records fall outside poly_shape's boundaries.\n",
    "    if expand_polygon_bool:\n",
    "        outside = []\n",
    "        for record in records:\n",
    "            if not poly_shape.contains(record.point):\n",
    "                outside.append(record.point)\n",
    "        outside_rate = len(outside) / len(records)\n",
    "        if verbose:\n",
    "            print('# crashes: {}, # records outside {}'.format(len(records), len(outside)))\n",
    "\n",
    "        # If too many points fell outside, buffer [enlarge] the poly_shape until less than max_percent fall.\n",
    "        while outside_rate > max_percent:\n",
    "            print(\"{}% of crashes fell outside the city polygon so we are buffering\".format(int(round(outside_rate, 2) * 100)))\n",
    "            poly_shape, num_out = buffer_polygon(poly_shape, outside, distMax=10000, verbose=verbose)\n",
    "            outside_rate = num_out / len(records)\n",
    "\n",
    "        # Again write the shapefile to directory so we can inspect how it has changed\n",
    "        with fiona.open(os.path.join(MAP_DIR, \"polygon_coords_buffered.shp\"), \"w\", \"ESRI Shapefile\", schema) as output:\n",
    "            output.write({\n",
    "                'geometry': mapping(poly_shape),\n",
    "                'properties': {'id': 123}\n",
    "            })\n",
    "\n",
    "    else:\n",
    "        print('NOTE: expand_poly_bool = False')\n",
    "        print('Thus we are not attempting to expand polygon. Normally would check if all crash data falls within polygon, and if not expand.')\n",
    "        print('Expanding takes a long time however, so can skip if you are sure your polygon from OSM is large enough.')\n",
    "\n",
    "    # Convert back to 4326 projection\n",
    "    coords = [util.get_reproject_point(\n",
    "        x[1],\n",
    "        x[0],\n",
    "        inproj='epsg:3857',\n",
    "        outproj='epsg:4326',\n",
    "        coords=True\n",
    "    ) for x in poly_shape.exterior.coords]\n",
    "    poly_shape_4326 = Polygon(coords)\n",
    "\n",
    "    return poly_shape_4326\n",
    "\n",
    "\n",
    "def buffer_polygon(polygon, points, distMax=10000, verbose=False):\n",
    "    \"\"\"\n",
    "    Given a set of points outside a polygon, expand the polygon\n",
    "    to include points within 250 meters\n",
    "    Args:\n",
    "        polygon - shapely polygon\n",
    "        points - list of shapely points\n",
    "    Returns:\n",
    "        new polygon with buffered points added\n",
    "    \"\"\"\n",
    "    not_close = []\n",
    "    add_buffers = []\n",
    "    len_points = len(points)\n",
    "    counter = 0\n",
    "\n",
    "    # Find the distance between points and exterior of city polygon\n",
    "    # If they are within distMax from the exterior, include them in the polygon, otherwise leave out.\n",
    "    poly_ext = LinearRing(polygon.exterior.coords)\n",
    "    for point in points:\n",
    "        dist = polygon.distance(point)\n",
    "        if dist > distMax:\n",
    "            not_close.append(point)\n",
    "        else:\n",
    "            point2 = poly_ext.interpolate(poly_ext.project(point))\n",
    "            line = LineString([(point.x, point.y), (point2.x, point2.y)])\n",
    "            buff = line.buffer(50)\n",
    "            add_buffers.append(buff)\n",
    "        if verbose:\n",
    "            print(\"{} / {} - {} % buffered\".format(counter, len_points, 100 * counter / len_points))\n",
    "            counter += 1\n",
    "\n",
    "    # Now that all the buffers have been discovered, we create our new polygon as a union.\n",
    "    for buff in add_buffers:\n",
    "        polygon = polygon.union(buff)\n",
    "\n",
    "    # Check how many polygons were still not included and return as a percentage\n",
    "    num_out = len(not_close)\n",
    "    if not_close:\n",
    "        print(\"{} crashes fell outside the buffered city polygon\".format(len(not_close)))\n",
    "    else:\n",
    "        print(\"Expanded city polygon to include all crash locations\")\n",
    "\n",
    "    return polygon, num_out\n",
    "\n",
    "\n",
    "def simple_get_roads(config, verbose, expand_poly=False):\n",
    "    \"\"\"\n",
    "    Use osmnx to get a simplified version of open street maps for the city\n",
    "    Writes osm_nodes and osm_ways shapefiles to MAP_DIR\n",
    "    Args:\n",
    "        city\n",
    "    Returns:\n",
    "        None\n",
    "    Creates:\n",
    "           osm_ways.shp - the simplified road network\n",
    "           osm_nodes.shp - the intersections and dead ends\n",
    "    Creates directory:\n",
    "           all_nodes - containing edges and nodes directories for the unsimplified road network\n",
    "    \"\"\"\n",
    "\n",
    "    # Load in polygon from request to nominatim.openstreetmap.org/search\n",
    "    # Polygon takes the form {'type' : 'Polygon', 'coordinates': [[[x, y], [x2, y2], ...]]}\n",
    "    print(\"searching OSM for \" + str(config['city']) + \" polygon\")\n",
    "    polygon_pos, polygon = find_osm_polygon(config['city'])\n",
    "    if verbose:\n",
    "        print('Polygon_pos variables is:', polygon_pos)\n",
    "        print('Polygon type is:', type(polygon))\n",
    "\n",
    "    # If too many crashes land outside of our city polygon, expand the polygon\n",
    "    if expand_poly:\n",
    "        print('Expand_poly is true. About to attempt to expand the polygon to include more crashes')\n",
    "        polygon = expand_polygon(polygon, os.path.join(PROCESSED_CRASH_DIR, 'crashes.json'), verbose)\n",
    "    else:\n",
    "        polygon = Polygon(polygon['coordinates'][0])\n",
    "\n",
    "    # Creates a network graph from OSM data within the spatial boundaries of the passed-in shapely polygon\n",
    "    print('Creating a graph from polygon data')\n",
    "    G1 = ox.graph_from_polygon(polygon, network_type='drive', simplify=False)\n",
    "\n",
    "    # Simplify graph's topology by removing all nodes that are not intersections or dead-ends.\n",
    "    # Creates an edge directly between endpoints that encapsulate them, but retain the geometry of original edges,\n",
    "    # saved as attribute in new edge.\n",
    "    # e.g. before a curved road would have many nodes for each part of curve. Now only a node at beginning and end, but curvature is retained.\n",
    "    print('Simplifying graph by removing nodes that are not intersections or dead-ends')\n",
    "    G = ox.simplify_graph(G1)\n",
    "\n",
    "    # Label dead ends here as must be done before saving object out, after which cant use count_streets_per_node\n",
    "    # Will finish cleanup of node data during the geojson write-out.\n",
    "    # Add dead_end to the node properties\n",
    "    print('Labelling dead ends')\n",
    "    streets_per_node = ox.count_streets_per_node(G)\n",
    "    for node, count in list(streets_per_node.items()):\n",
    "        if count <= 1:\n",
    "            G.nodes()[node]['dead_end'] = True\n",
    "\n",
    "    # save_graph_shapefile saves graph nodes and edges as ESRI shapefile\n",
    "    # Save both simplified and complex graphs. The graphs extra information can be used as features.\n",
    "    # Saving outputs nodes and edges folders with nodes / edges files of the format .cpg, .dbf, .prj, .shp, .shx\n",
    "    # The edges have the following properties directly from OSM: access, bridge, from, highway, junction, key, lanes, length, maxspeed, name, oneway, osmid, ref, service, to, width\n",
    "    # The nodes have the following properties directly from OSM: higway, osmid. 'dead_end' is added manually above.\n",
    "    print('Saving graph that has all nodes which may be later used as features')\n",
    "    ox.save_graph_shapefile(G1, filename='all_nodes', folder=MAP_DIR)\n",
    "    print('Saving simplified graph')\n",
    "    ox.save_graph_shapefile(G, filename='temp', folder=MAP_DIR)\n",
    "\n",
    "    # Copy the simplified files in the temp directory, moving them to the MAP_DIR directory\n",
    "    # Label those files that were put in the edges directory as osm_ways.ext\n",
    "    # Label those files that were put in the nodes directory as osm_nodes.ext\n",
    "    tempdir = os.path.join(MAP_DIR, 'temp')\n",
    "\n",
    "    for filename in os.listdir(os.path.join(tempdir, 'edges')):\n",
    "        _, extension = filename.split('.')\n",
    "        shutil.move(os.path.join(tempdir, 'edges', filename), os.path.join(MAP_DIR, 'osm_ways.' + extension))\n",
    "\n",
    "    for filename in os.listdir(os.path.join(tempdir, 'nodes')):\n",
    "        _, extension = filename.split('.')\n",
    "        shutil.move(os.path.join(tempdir, 'nodes', filename), os.path.join(MAP_DIR, 'osm_nodes.' + extension))\n",
    "\n",
    "    shutil.rmtree(tempdir)\n",
    "\n",
    "\n",
    "def clean_and_write(ways_file, nodes_file, result_file, DOC_DIR, verbose):\n",
    "    \"\"\"\n",
    "    Takes several shape files in 4326 projection, created from osmnx, reprojects them, and calls write_geojson\n",
    "    Args:\n",
    "        ways_file - shp file for the ways\n",
    "        nodes_file - shp file for the intersection and end nodes\n",
    "        all_nodes_file - shp file for ALL nodes in the road network\n",
    "        result_file - file to write to\n",
    "        DOC_DIR - file to write highway keys to\n",
    "    Returns:\n",
    "        OSM_elements.geojson [or whatever result_file name you are using]\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print('Within osm_create_maps.clean_and_write with the following variables')\n",
    "        print('ways_file:', ways_file)\n",
    "        print('nodes_file:', nodes_file)\n",
    "        print('results_file:', result_file)\n",
    "        print('DOC_DIR:', DOC_DIR)\n",
    "\n",
    "    # Reads osm_ways file, cleans up features and reprojects onto 3875\n",
    "    # Additionally writes mapping for highway types and saves to DOC_DIR\n",
    "    # cleaned_ways include way objects which are ordered dictionaries with the following keys:\n",
    "    # access, area, birdge, est_width, from, highway, junction, key, lanes, length, maxspeed, name, oneway, osmid, ref, service, to, tunnel, width, hwy_type, osm_speed, signal, width_per_lane\n",
    "    cleaned_ways = clean_ways(ways_file, DOC_DIR, verbose)\n",
    "\n",
    "    # Populate the cross streets for each node and add unique ids to the ways\n",
    "    # Returns nodes [same as shape file but with the added properties of 'streets', 'intersection', 'signal'] and ways [with unique osmid-fromNodeNumber-toNodeNumber string]\n",
    "    # Each node in nodes comes with the following keys: dead_end, highway, osmid, ref\n",
    "    nodes = fiona.open(nodes_file)\n",
    "    nodes, cleaned_ways = populate_node_features_ways_ID(cleaned_ways, nodes, verbose)\n",
    "\n",
    "    # Append nodes to ways and output all into a result_file [in this case osm_elements.geojson]\n",
    "    write_geojson(cleaned_ways, nodes, result_file)\n",
    "\n",
    "\n",
    "def clean_ways(orig_file, DOC_DIR, verbose):\n",
    "    \"\"\"\n",
    "    Reads in osm_ways file, cleans up the features, and reprojects\n",
    "    results into 3857 projection. Additionally writes a key which shows the correspondence\n",
    "    between highway type as a string and the resulting int feature\n",
    "    Features:\n",
    "        width\n",
    "        lanes\n",
    "        hwy_type\n",
    "        osm_speed\n",
    "        signal\n",
    "    Args:\n",
    "        orig_file: Filename for original file\n",
    "        result_file: Filename for resulting file in 3857 projection\n",
    "        DOC_DIR: directory to write highway keys file to\n",
    "    Returns:\n",
    "        a list of reprojected way lines\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print('Within osm_create_maps.clean_ways with the following variables')\n",
    "        print('orig file:', orig_file)\n",
    "\n",
    "    way_lines = fiona.open(orig_file)\n",
    "\n",
    "    highway_keys = {}\n",
    "    results = []\n",
    "\n",
    "    # The way_line comes with the following properties from the shp file:\n",
    "    # access, area, bridge, est_width, from, highway, junction, key, lanes,\n",
    "    # length, maxspeed, name, oneway, osmid, ref, service, to, tunnel, width\n",
    "    for way_line in way_lines:\n",
    "\n",
    "        # Get speed for each way. Assign 0 if not found.\n",
    "        speed = get_speed(way_line['properties']['maxspeed']) if 'maxspeed' in list(way_line['properties']) else 0\n",
    "\n",
    "        # Get width for each way. Assign 0 if not found.\n",
    "        width = get_width(way_line['properties']['width']) if 'width' in list(way_line['properties']) else 0\n",
    "\n",
    "        # Get lanes for each way\n",
    "        lanes = way_line['properties']['lanes']\n",
    "        if lanes:\n",
    "            lanes = max([int(x) for x in re.findall('\\d', lanes)])\n",
    "        else:\n",
    "            lanes = 0\n",
    "\n",
    "        # Get one-way for each way\n",
    "        oneway = 0\n",
    "        if way_line['properties']['oneway'] == 'True':\n",
    "            oneway = 1\n",
    "\n",
    "        # Get the approximate directionality of each way\n",
    "        # get_direction returns 'N-S', 'E-W', 'NE-SW', 'NW-SE' for directions\n",
    "        way_coords = way_line['geometry']['coordinates']\n",
    "        way_start = way_coords[0]\n",
    "        way_end = way_coords[1]\n",
    "        way_path_x = (way_end[0] - way_start[0])\n",
    "        way_path_y = (way_end[1] - way_start[1])\n",
    "        direction = get_direction(way_path_x, way_path_y)\n",
    "\n",
    "        # Derive highway mapping\n",
    "        if way_line['properties']['highway'] not in list(highway_keys.keys()):\n",
    "            highway_keys[way_line['properties']['highway']] = len(highway_keys)\n",
    "\n",
    "        # Derive width per lane\n",
    "        width_per_lane = 0\n",
    "        if lanes and width:\n",
    "            width_per_lane = round(width / lanes)\n",
    "\n",
    "        # Update properties with newly found variables\n",
    "        # Note we include 'signal':0, but will properly update this varible later in write_geojson\n",
    "        way_line['properties'].update({\n",
    "            'width': width,\n",
    "            'lanes': int(lanes),\n",
    "            'hwy_type': highway_keys[way_line['properties']['highway']],\n",
    "            'osm_speed': speed,\n",
    "            'signal': 0,\n",
    "            'oneway': oneway,\n",
    "            'width_per_lane': width_per_lane,\n",
    "            'direction': direction\n",
    "        })\n",
    "\n",
    "        results.append(way_line)\n",
    "\n",
    "    # Write the highway keys to a mapping document in processed/mapping\n",
    "    write_highway_keys(DOC_DIR, highway_keys)\n",
    "\n",
    "    # Returns our results, which is an ordered dict of ways with the following properties:\n",
    "    # access, area, birdge, est_width, from, highway, junction, key, lanes, length, maxspeed\n",
    "    # name, oneway, osmid, ref, service, to, tunnel, width, hwy_type, osm_speed, signal, width_per_lane\n",
    "    return results\n",
    "\n",
    "\n",
    "def populate_node_features_ways_ID(ways, nodes, verbose):\n",
    "    \"\"\"\n",
    "    For each node the 'streets' [incoming street names], 'intersection' [dead-end or intersection 0 / 1] and 'signal' [traffic lights or not 0 / 1] properties\n",
    "    For each way create a unique ID based on the nodes it spans and the way ID\n",
    "\n",
    "    Args:\n",
    "        ways - a list of geojson linestrings\n",
    "        nodes - a list of geojson points\n",
    "    Returns:\n",
    "        nodes - a dict containing the roads connected to each node\n",
    "        ways - the ways, with a unique osmid-fromNodeID-toNodeID string\n",
    "    \"\"\"\n",
    "\n",
    "    # node_info will hold a key for each node, and record any roads connected to the node.\n",
    "    node_info = {}\n",
    "    for way in ways:\n",
    "\n",
    "        # There are some collector roads and others that don't have names. Skip these.\n",
    "        # For all the others, we fill out our node_info dictionary so that for a given node it lists all the incoming road names\n",
    "        if way['properties']['name']:\n",
    "\n",
    "            # While we are still merging segments with different names, just use both roads. Revisit.\n",
    "            # E.g. [Munz Avenue, Telfast Street] -> 'Munz Avenue/Telfast Street'\n",
    "            if '[' in way['properties']['name']:\n",
    "                way['properties']['name'] = re.sub(r'[^\\s\\w,]|_', '', way['properties']['name'])\n",
    "                way['properties']['name'] = \"/\".join(way['properties']['name'].split(', '))\n",
    "\n",
    "            # Check if the 'from' property is in the node_info keys. If it's not, create an entry for that node number.\n",
    "            # Append to the node number the name of the street that is attached\n",
    "            if way['properties']['from'] not in node_info.keys():\n",
    "                node_info[way['properties']['from']] = []\n",
    "            node_info[way['properties']['from']].append(way['properties']['name'])\n",
    "\n",
    "            # Check if the 'to' property is in the node_info keys. If it's not, create an entry for that node number.\n",
    "            # Append to the node number the name of the street it is going to\n",
    "            if way['properties']['to'] not in node_info.keys():\n",
    "                node_info[way['properties']['to']] = []\n",
    "            node_info[way['properties']['to']].append(way['properties']['name'])\n",
    "\n",
    "        # Add a unique identifier to way['segment_id'] which takes the form OSMID - NodeNumberFrom - NodeNumberTo\n",
    "        ident = str(way['properties']['osmid']) + '-' + str(way['properties']['from']) + '-' + str(way['properties']['to'])\n",
    "        way['properties']['segment_id'] = ident\n",
    "\n",
    "    # Add the 'streets', 'intersection' and 'signal' properties to our nodes\n",
    "    # Immediately previously filled out node_info to detail the incoming roads for each node\n",
    "    # Now append this extra data into our nodes object, creating nodes_extra_features\n",
    "    # The inbuilt 'set' function is used to eliminate duplicates\n",
    "    nodes_extra_features = []\n",
    "    for node in nodes:\n",
    "\n",
    "        # Adding 'streets' property\n",
    "        if node['properties']['osmid'] in node_info:\n",
    "            node['properties']['streets'] = ', '.join(set(node_info[node['properties']['osmid']]))\n",
    "        else:\n",
    "            node['properties']['streets'] = ''\n",
    "\n",
    "        # Adding 'dead_end' property\n",
    "        if not node['properties']['dead_end']:\n",
    "            node['properties']['intersection'] = 1\n",
    "\n",
    "        # Adding 'signal' property\n",
    "        if node['properties']['highway'] == 'traffic_signals':\n",
    "            node['properties']['signal'] = 1\n",
    "\n",
    "        nodes_extra_features.append(node)\n",
    "\n",
    "    return nodes_extra_features, ways\n",
    "\n",
    "\n",
    "def write_geojson(way_results, node_results, outfp):\n",
    "    \"\"\"\n",
    "    Given a list of ways, intersection nodes, and all nodes, write them out to a geojson file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get all the ways\n",
    "    feats = way_results\n",
    "\n",
    "    # Get all the nodes. Check if they have traffic lights / are a dead end and edit properties\n",
    "    # Add the nodes to the feats, so that it contains nodes and ways\n",
    "    for node in node_results:\n",
    "        feats.append(geojson.Feature(geometry=geojson.Point(node['geometry']['coordinates']), properties=node['properties']))\n",
    "\n",
    "    # Write out features.\n",
    "    feat_collection = geojson.FeatureCollection(feats)\n",
    "    with open(outfp, 'w') as outfile:\n",
    "        geojson.dump(feat_collection, outfile)\n",
    "\n",
    "\n",
    "def write_highway_keys(DOC_DIR, highway_keys):\n",
    "    \"\"\"\n",
    "    Since we're creating a numeric highway key, we'd like to know what\n",
    "    the numbers correspond to, so write to file the mapping from key\n",
    "    to open street map highway type\n",
    "    Args:\n",
    "        DOC_DIR - the directory to write the file\n",
    "        highway_keys - a dict associating key with string type\n",
    "    \"\"\"\n",
    "    # Write highway keys to docs if needed for reference\n",
    "    if not os.path.exists(DOC_DIR):\n",
    "        os.makedirs(DOC_DIR)\n",
    "    with open(os.path.join(DOC_DIR, 'highway_keys.csv'), 'w') as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow(['type', 'value'])\n",
    "        for item in highway_keys.items():\n",
    "            w.writerow(item)\n",
    "\n",
    "\n",
    "def get_width(width):\n",
    "    \"\"\"\n",
    "    Parse the width from the openstreetmap width property field\n",
    "    Args:\n",
    "        width - a string\n",
    "    Returns:\n",
    "        width - an int\n",
    "    \"\"\"\n",
    "\n",
    "    # This indicates two segments combined together.\n",
    "    # For now, we just skip combined segments with different widths\n",
    "    if not width or ';' in width or '[' in width:\n",
    "        width = 0\n",
    "    else:\n",
    "        # Sometimes there's bad (non-numeric) width\n",
    "        # so remove anything that isn't a number or .\n",
    "        # Skip those that don't have some number in them\n",
    "        width = re.sub('[^0-9\\.]+', '', width)\n",
    "        if width:\n",
    "            width = round(float(width))\n",
    "        else:\n",
    "            width = 0\n",
    "    return width\n",
    "\n",
    "\n",
    "def get_speed(speed):\n",
    "    \"\"\"\n",
    "    Parse the speed from the openstreetmap maxspeed property field\n",
    "    If there's more than one speed (from merged ways), use the highest speed\n",
    "    Args:\n",
    "        speed - a string\n",
    "    Returns:\n",
    "        speed - an int\n",
    "    \"\"\"\n",
    "    if speed:\n",
    "        speeds = [int(x) for x in re.findall('\\d+', speed)]\n",
    "        if speeds:\n",
    "            return max(speeds)\n",
    "    return 0\n",
    "\n",
    "\n",
    "def write_features(all_nodes_file):\n",
    "    \"\"\"\n",
    "    Adds relevant features from open street maps\n",
    "    \"\"\"\n",
    "\n",
    "    all_node_results = fiona.open(all_nodes_file)\n",
    "    features = []\n",
    "\n",
    "    # Go through the rest of the nodes, and add any of them that have\n",
    "    # (hardcoded) open street map features that we care about\n",
    "    # For the moment, all_nodes only contains street nodes, so we'll\n",
    "    # only look at signals and crosswalks\n",
    "    for node in all_node_results:\n",
    "\n",
    "        if node['properties']['highway'] == 'crossing':\n",
    "            features.append(geojson.Feature(geometry=geojson.Point(node['geometry']['coordinates']), id=node['properties']['osmid'], properties={'feature': 'crosswalk'}))\n",
    "\n",
    "        elif node['properties']['highway'] == 'traffic_signals':\n",
    "            features.append(geojson.Feature(geometry=geojson.Point(node['geometry']['coordinates']), id=node['properties']['osmid'], properties={'feature': 'signal'}))\n",
    "\n",
    "    features = geojson.FeatureCollection(features)\n",
    "\n",
    "    with open(os.path.join(MAP_DIR, 'features.geojson'), \"w\") as f:\n",
    "        json.dump(features, f)\n",
    "\n",
    "\n",
    "def get_direction(x, y):\n",
    "    # Pass in a vector with direction (x, y), return the general direction that vector runs along\n",
    "\n",
    "    # Numpy convention uses y, x for arctan.\n",
    "    angle = np.arctan2(y, x)\n",
    "\n",
    "    # Convert angle to degrees for easier visualisation\n",
    "    angle = angle * 180 / np.pi\n",
    "\n",
    "    # 0 is along the positive x-axis, 90 along the positive y-axis, -90 along the negative y-axis etc.\n",
    "    # Splitting the angles between E-W [east-west], NE-SW (northeast-southwest), N-S (north-south) and NW-SE (northwest-southeast)\n",
    "    angles = [-157.5, -112.5, -67.5, -22.5, 22.5, 67.5, 112.5, 157.5]\n",
    "\n",
    "    if angles[0] < angle < angles[1] or angles[4] < angle < angles[5]:\n",
    "        return 'NE-SW'\n",
    "    elif angles[1] < angle < angles[2] or angles[5] < angle < angles[6]:\n",
    "        return 'N-S'\n",
    "    elif angles[2] < angle < angles[3] or angles[6] < angle < angles[7]:\n",
    "        return 'NW-SE'\n",
    "    elif angles[3] < angle < angles[4] or angle > angles[7] or angle < angles[0]:\n",
    "        return 'E-W'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(MAP_DIR, 'osm_ways.shp')):\n",
    "    print('Generating map from open street map...')\n",
    "    simple_get_roads(config, True)\n",
    "\n",
    "# Begin by cleaning up ways data, adding some derived features [e.g. new ID field], finding which roads intersect which nodes\n",
    "# Then write out osm_elements.geojson as a combination of osm_ways and osm_nodes.\n",
    "if not os.path.exists(os.path.join(MAP_DIR, 'osm_elements.geojson')):\n",
    "    print(\"Cleaning and writing to {}...\".format('osm_elements.geojson'))\n",
    "\n",
    "    clean_and_write(\n",
    "        os.path.join(MAP_DIR, 'osm_ways.shp'),\n",
    "        os.path.join(MAP_DIR, 'osm_nodes.shp'),\n",
    "        os.path.join(MAP_DIR, 'osm_elements.geojson'),\n",
    "        DOC_DIR, True\n",
    "    )\n",
    "\n",
    "# Look through the full all_nodes shp file.\n",
    "# Add features for any traffic signals or road crossings found at these nodes\n",
    "# Write out these features to features.geojson\n",
    "if not os.path.exists(os.path.join(MAP_DIR, 'features.geojson')) or args.forceupdate:\n",
    "    print('Writing out features.geojson from all_nodes')\n",
    "    all_nodes_path = os.path.join(MAP_DIR, 'all_nodes', 'nodes', 'nodes.shp')\n",
    "    write_features(all_nodes_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rtree\n",
    "import json\n",
    "import copy\n",
    "from shapely.ops import unary_union\n",
    "from collections import defaultdict\n",
    "import argparse\n",
    "import os\n",
    "import geojson\n",
    "import re\n",
    "import sys\n",
    "from shapely.geometry import MultiLineString, LineString\n",
    "\n",
    "import util\n",
    "from segment import Segment\n",
    "\n",
    "\n",
    "def create_segments_from_json(OSM_elements_path, MAP_DIR):\n",
    "    # Returns non_int_w_ids and union_inter\n",
    "    # non_int_w_ids are all the segments that do not have an intersection\n",
    "    # union_inter are intersection segments where adjacent elements have been combined\n",
    "\n",
    "    # Return roads [features with type = 'LineString'] and intersections [features with 'intersection' = 1]\n",
    "    # We use roads_shp_path = osm_elements.geojson normally\n",
    "    print(\"Within create_segments.create_segments_from_json\")\n",
    "    roads, inters = util.get_roads_and_inters(OSM_elements_path)\n",
    "    print(\"Read in {} road segments\".format(len(roads)))\n",
    "\n",
    "    # Unique ID did not get included in shapefile, need to add it for adjacency\n",
    "    for i, road in enumerate(roads):\n",
    "        road.properties['orig_id'] = int(str(99) + str(i))\n",
    "\n",
    "    # Creates a list that takes the form {ID: 'street names'}\n",
    "    # If no OSM ID is present, set the key to 0. This situation may occur\n",
    "    # when we generate alternate maps from city data\n",
    "    inters_by_id = {\n",
    "        x['properties']['osmid'] if 'osmid' in x['properties'] else '0':\n",
    "        x['properties']['streets'] if 'streets' in x['properties'] else None\n",
    "        for x in inters\n",
    "    }\n",
    "\n",
    "    # Buffer each intersection by 20 meters\n",
    "    int_buffers = get_intersection_buffers(inters, 20)\n",
    "    print(\"Found {} intersection buffers\".format(len(int_buffers)))\n",
    "\n",
    "    # Take in the roads and the intersection buffers just found\n",
    "    # Output the parts of roads that do / dont overlap with the buffers\n",
    "    non_int_lines, inter_segments = split_segments(roads, int_buffers)\n",
    "\n",
    "    # For road lines that do not overlap with intersection, rejig some internal variables.\n",
    "    # In particular add the following properties:\n",
    "    # 'id' = ('00' + the row of road), 'inter' (= 0 as these are roads), 'displayname'\n",
    "    # Give them a display_name and append the result to non_int_w_ids\n",
    "    non_int_w_ids = []\n",
    "    for i, line in enumerate(non_int_lines):\n",
    "        value = copy.deepcopy(line)\n",
    "        value['type'] = 'Feature'\n",
    "        value['properties']['id'] = '00' + str(i)\n",
    "        value['properties']['inter'] = 0\n",
    "        value['properties']['display_name'] = get_non_intersection_name(line, inters_by_id)\n",
    "\n",
    "        x, y = util.get_center_point(value)\n",
    "        x, y = util.reproject([[x, y]], inproj='epsg:3857', outproj='epsg:4326')[0]['coordinates']\n",
    "        value['properties']['center_y'] = y\n",
    "        value['properties']['center_x'] = x\n",
    "\n",
    "        non_int_w_ids.append(value)\n",
    "\n",
    "    print(\"Extracted {} non-intersection segments\".format(len(non_int_w_ids)))\n",
    "\n",
    "    # Create union_inter, which consists taking the union of road segments that intersect with intersection buffers\n",
    "    # In particular, 'id', 'data' and 'display_name' properties are added\n",
    "    # Planarize intersection segments, turning the list of LineStrings into a MultiLineString\n",
    "    # Previously, the elements of inter_segments['lines'] may have been multiple shapely lines.\n",
    "    union_inter = []\n",
    "    print('Now taking the union of segments that overlap an intersection buffer')\n",
    "    print('Note during this step we set intersection=1 for the segments. Investigate why it was not set earlier')\n",
    "    for idx, lines in list(inter_segments['lines'].items()):\n",
    "\n",
    "        lines = unary_union(lines)\n",
    "        coords = []\n",
    "\n",
    "        # Fix some mislabelling of dead-end node as intersection due to issue with OSMX\n",
    "        if type(lines) == LineString:\n",
    "            lines = MultiLineString([lines.coords])\n",
    "        for line in lines:\n",
    "            coords += [[x for x in line.coords]]\n",
    "\n",
    "        # Assign name to intersection\n",
    "        name = get_intersection_name(inter_segments['data'][idx])\n",
    "\n",
    "        # Add the number of segments coming into this intersection to the segment_data\n",
    "        # Also add the display name of the intersection to each of the segments\n",
    "        # Also manually set intersection to 1. Should have already been this, but wasn't.\n",
    "        # Not sure why, look into it later.\n",
    "        segment_data = []\n",
    "        for segment in list(inter_segments['data'][idx]):\n",
    "            segment['intersection_segments'] = len(inter_segments['data'][idx])\n",
    "            segment['intersection'] = 1\n",
    "            segment['display_name'] = name\n",
    "            segment_data.append(segment)\n",
    "\n",
    "        # Include the id and display name here also. This is historic, and only left to make sure nothing breaks\n",
    "        # Makes our value have the form {'id': 00, 'data': {... display_name: 'punt rd' ...}, 'display_name': 'punt rd'}\n",
    "        properties = {\n",
    "            'id': idx,\n",
    "            'data': segment_data,\n",
    "            'display_name': name\n",
    "        }\n",
    "\n",
    "        value = geojson.Feature(geometry=geojson.MultiLineString(coords), id=idx, properties=properties)\n",
    "\n",
    "        x, y = util.get_center_point(value)\n",
    "        x, y = util.reproject([[x, y]], inproj='epsg:3857', outproj='epsg:4326')[0]['coordinates']\n",
    "        value['properties']['center_x'] = x\n",
    "        value['properties']['center_y'] = y\n",
    "\n",
    "        union_inter.append(value)\n",
    "\n",
    "    return non_int_w_ids, union_inter\n",
    "\n",
    "\n",
    "def get_intersection_buffers(intersections, intersection_buffer_units, debug=False, load_buffer_path=False):\n",
    "    \"\"\"\n",
    "    Buffers intersection according to proj units\n",
    "    Args:\n",
    "        intersections\n",
    "        intersection_buffer_units - in meters\n",
    "        debug - if true, will output the buffers to file for debugging\n",
    "    Returns:\n",
    "        a list of polygons, buffering the intersections\n",
    "        these are circles, or groups of overlapping circles\n",
    "    \"\"\"\n",
    "\n",
    "    print('Within create_segments.get_intersection_buffers.')\n",
    "    print('Many intersections are not correctly labelled by OSM.')\n",
    "\n",
    "    if not load_buffer_path:\n",
    "        # Uses .buffer from shapely to buffer the intersection geometry, creating buffered polygons around all intersections\n",
    "        buffered_intersections = [intersection['geometry'].buffer(intersection_buffer_units) for intersection in intersections]\n",
    "\n",
    "        # Uses unary_union from shapely to get the union of the buffered circles\n",
    "        buffered_intersections = unary_union(buffered_intersections)\n",
    "        print('Finished buffering intersections.')\n",
    "    else:\n",
    "        with open(load_buffer_path, \"rb\") as fp:\n",
    "            buffered_intersections = pickle.load(fp)\n",
    "\n",
    "    # Output the buffered shapes for troubleshooting\n",
    "    if debug:\n",
    "        util.output_from_shapes(\n",
    "            [(x, {}) for x in buffered_intersections],\n",
    "            os.path.join(MAP_DIR, 'int_buffers.geojson')\n",
    "        )\n",
    "\n",
    "    # Index the intersection points for for faster lookup in subsequent sections\n",
    "    print('Creating a spatial index to allow for faster lookups of intersections')\n",
    "    inter_index = rtree.index.Index()\n",
    "    for idx, inter_point in enumerate(intersections):\n",
    "        inter_index.insert(idx, inter_point['geometry'].bounds)\n",
    "\n",
    "    # Get the intersections that overlap with other intersection buffers and collating, appending to results\n",
    "    print('Checking which intersections overlap buffers and creating results list of the form [[buffer, matching_intersections]]')\n",
    "    results = []\n",
    "    for buff in buffered_intersections:\n",
    "        matches = []\n",
    "        for idx in inter_index.intersection(buff.bounds):\n",
    "            if intersections[idx]['geometry'].within(buff):\n",
    "                matches.append(intersections[idx]['geometry'])\n",
    "        results.append([buff, matches])\n",
    "    print('Finished associating intersections with buffers.')\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def split_segments(roads, int_buffers):\n",
    "    \"\"\"\n",
    "    Finds which parts of each segment either overlaps with a buffered intersection,\n",
    "    or doesn't. Returns non_int_lines and inter_segments\n",
    "    Args:\n",
    "        roads - a list of tuples of shapely shape and dict of segment info\n",
    "        int_buffers - a list of polygons that buffer intersections\n",
    "    Returns:\n",
    "        tuple consisting of:\n",
    "            non_int_lines - list in same format as input roads, just a subset\n",
    "                Each element in the list is a tuple of LineString or MultiLineString and dict of properties\n",
    "\n",
    "            inter_segments - dict of lists with keys 'data' and 'lines'.\n",
    "                Each element in the lines list is one of the lines overlapping the intersection buffer\n",
    "                Each element in the data list is a dict of properties corresponding to the lines\n",
    "    \"\"\"\n",
    "\n",
    "    # Create buffered_lines to hold tuples with buffer and original road\n",
    "    # Also update the index with the index number and buffer bounds.\n",
    "    # Road_lines_index will be used to find those roads and their idx that intersect with int_buffers\n",
    "    road_lines_index = rtree.index.Index()\n",
    "    buffered_lines = []\n",
    "    for idx, road in enumerate(roads):\n",
    "        b = road.geometry.buffer(20)\n",
    "        buffered_lines.append((b, road))\n",
    "        road_lines_index.insert(idx, b.bounds)\n",
    "\n",
    "    # Setting up init parameters to be filled.\n",
    "    roads_with_int_segments = {}\n",
    "    inter_segments = {'lines': defaultdict(list), 'data': defaultdict(list)}\n",
    "    count = 0\n",
    "\n",
    "    # Iterate through buffered intersections\n",
    "    # Check if any buffered road intersect with buffered intersection\n",
    "    # If intersect add road to matched_roads, and add intersecting segment to match_segments\n",
    "    # Then return roads_with_int_Segments which tracks what intersections roads cross with\n",
    "    # Util.track prints progress every 1000 steps\n",
    "    print(\"Inside create_segments.split_segments. Generating intersection segments\")\n",
    "    for i, int_buffer in enumerate(int_buffers):\n",
    "        util.track(i, 10000, len(int_buffers))\n",
    "        match_segments = []\n",
    "        matched_roads = []\n",
    "        for idx in road_lines_index.intersection(int_buffer[0].bounds):\n",
    "            road = roads[idx]\n",
    "            match_segments.append(Segment(road.geometry.intersection(int_buffer[0]), road.properties))\n",
    "            matched_roads.append(road)\n",
    "\n",
    "        # Connect road segments to intersections, combine close intersections\n",
    "        # Match_segments looks like [segment object 1, segment object 2]\n",
    "        # int_segments looks like [({segment object 1, segment object 2}, union(segments))]\n",
    "        # Returns tuple containing set of (set(segments), buffer(unary_unions(segments)))\n",
    "        int_segments = get_connections(int_buffer[1], match_segments)\n",
    "\n",
    "        # For each road that intersects with the intersection, create an entry in roads_with_int_segments, attaching int_segments.\n",
    "        # roads_with_int_segments looks like {'id': [({segment object 1, segment object 2}, unions(segments)), the same again], 'id2: ...}\n",
    "        # road_with_int_segments is initialised above the loop\n",
    "        for r in matched_roads:\n",
    "            if r.properties['id'] not in roads_with_int_segments:\n",
    "                roads_with_int_segments[r.properties['id']] = []\n",
    "            roads_with_int_segments[r.properties['id']] += int_segments\n",
    "\n",
    "        # For each segment in the tuple of int_segments:\n",
    "        # Add the geometry and properties of the segment to inter_segments['lines' / 'data']\n",
    "        # inter_segments is initialised above the loop\n",
    "        for int_segment in int_segments:\n",
    "            inter_segments['lines'][count] = [x.geometry for x in int_segment[0]]\n",
    "            inter_segments['data'][count] = [x.properties for x in int_segment[0]]\n",
    "            count += 1\n",
    "\n",
    "    # For each road segment, check if it appears within roads_with_int_segments.\n",
    "    # If it doesn't, then append it to non_int_lines a.k.a non_int_roads as is, no chopping up needed\n",
    "    # If it does, then chop it up, and append the non-intersecting parts.\n",
    "    non_int_lines = []\n",
    "    print(\"Inside create_segments.split_segments. Generating non-intersection segments\")\n",
    "\n",
    "    for i, road in enumerate(roads):\n",
    "        util.track(i, 20000, len(roads))\n",
    "\n",
    "        # No overlap with intersection, append directly\n",
    "        if road.properties['id'] not in roads_with_int_segments:\n",
    "            non_int_lines.append(geojson.Feature(geometry=geojson.LineString([x for x in road.geometry.coords]), properties=road.properties))\n",
    "\n",
    "        # Overlaps with intersection, chop it up and append the non-intersecting area.\n",
    "        else:\n",
    "            road_info = roads_with_int_segments[road.properties['id']]\n",
    "            diff = road.geometry\n",
    "\n",
    "            # Get the difference between road geometry and the buffered intersection. This will be used to chop out the intersection\n",
    "            for inter in road_info:\n",
    "                buffered_int = inter[1]\n",
    "                diff = diff.difference(buffered_int)\n",
    "\n",
    "            # Do vector operations to remove the ovelapping section with the buffered intersection\n",
    "            if 'LineString' == diff.type:\n",
    "                non_int_lines.append(geojson.Feature(geometry=geojson.LineString([x for x in diff.coords]), properties=road.properties))\n",
    "\n",
    "            # Same idea for MultiLine string\n",
    "            elif 'MultiLineString' == diff.type:\n",
    "                coords = []\n",
    "                for l in diff:\n",
    "                    for coord in l.coords:\n",
    "                        coords.append(coord)\n",
    "                non_int_lines.append(geojson.Feature(geometry=geojson.LineString(coords), properties=road.properties))\n",
    "\n",
    "            # No sections of the segment fall outside the intersection, so skip.\n",
    "            else:\n",
    "                if len(diff) == 0:\n",
    "                    continue\n",
    "                print(\"{} found, skipping\".format(diff.type))\n",
    "\n",
    "    return non_int_lines, inter_segments\n",
    "\n",
    "\n",
    "def get_connections(points, segments):\n",
    "    \"\"\"\n",
    "    Gets intersections by looking at the connections between points\n",
    "    and segments that fall within an intersection buffer\n",
    "    Args:\n",
    "        points - a list of points\n",
    "        segments - a list of segment objects\n",
    "    Returns:\n",
    "        A list of tuples for each intersection.\n",
    "        Each tuple contains a set of segment objects\n",
    "        and the buffer of the unary_union of the segment objects\n",
    "        with a little bit of padding, because of a slight precision error\n",
    "        in shapely operations\n",
    "    \"\"\"\n",
    "    # Create a dict with each intersection point's coords as key\n",
    "    # The values are the point itself and an empty list that will\n",
    "    # store all the linestrings with a connection to the point\n",
    "    inters = []\n",
    "    for p in points:\n",
    "        inters.append([p, []])\n",
    "\n",
    "    # Get a starting list of all lines that touch any of the intersection points\n",
    "    # i.e. any line in segments that is within 0.0001m of an intersection point will be\n",
    "    # added to the intersection\n",
    "    for line in segments:\n",
    "        for i, (curr_shape, _) in enumerate(inters):\n",
    "            if line.geometry.distance(curr_shape) < .0001:\n",
    "                inters[i][1].append(line)\n",
    "                inters[i][0] = unary_union([inters[i][0], line.geometry])\n",
    "\n",
    "    # Merge connected components\n",
    "    # This section looks at all the intersections we have. It checks if any intersections overlap.\n",
    "    # For intersections which overlap, they, and all their lines, are connected and returned\n",
    "    resulting_inters = []\n",
    "    connected_lines = []\n",
    "    while inters:\n",
    "\n",
    "        # Gets the first intersection.\n",
    "        # Connected lines are fist defined as simply the intersection + joined roads\n",
    "        curr = inters.pop(0)\n",
    "        connected_lines = set(curr[1])\n",
    "\n",
    "        # Go through all intersections and check if any intersect with current intersection\n",
    "        if inters:\n",
    "            connected = [x[1] for x in inters if x[0].intersects(curr[0])]\n",
    "\n",
    "            # If any are connected, then add that to the set of connected lines\n",
    "            if connected:\n",
    "                connected_lines = set(curr[1] + [x for y in connected for x in y])\n",
    "\n",
    "        # Redefine intersection list to remove any already merged segments\n",
    "        inters = [x for x in inters if not x[0].intersects(curr[0])]\n",
    "\n",
    "        # Append to the result whichever intersection you just merged together\n",
    "        resulting_inters.append((connected_lines, unary_union([x.geometry for x in connected_lines]).buffer(.001)))\n",
    "\n",
    "    return resulting_inters\n",
    "\n",
    "\n",
    "def add_point_based_features(non_inters, inters, jsonfile, feats_filename=None,\n",
    "                             additional_feats_filename=None, forceupdate=False, verbose=False):\n",
    "    \"\"\"\n",
    "    Add any point-based set of features to existing segment data.\n",
    "    If it isn't already attached to the segments\n",
    "    Args:\n",
    "        non_inters\n",
    "        inters\n",
    "        jsonfile - points_joined.json, storing the results of snapping\n",
    "        feats_filename - geojson file for point-based features data\n",
    "        additional_feats_filename (optional) - file for additional\n",
    "            points-based data, in json format\n",
    "        forceupdate - if True, re-snap points and write to file\n",
    "    \"\"\"\n",
    "    print('Within create_segments.add_point_based_features')\n",
    "    print('Adding the extra features from total node map, plus any further added point features are included')\n",
    "\n",
    "    if forceupdate or not os.path.exists(jsonfile):\n",
    "        features = []\n",
    "        if feats_filename:\n",
    "            features = util.read_records_from_geojson(feats_filename)\n",
    "        if additional_feats_filename:\n",
    "            features += util.read_records(additional_feats_filename, 'record', verbose)\n",
    "\n",
    "        print('Snapping {} point-based features'.format(len(features)))\n",
    "        seg, segments_index = util.index_segments(\n",
    "            inters + non_inters\n",
    "        )\n",
    "\n",
    "        # Add ['properties']['near_id'] to features according to overlapping buffer zones and distances\n",
    "        util.find_nearest(features, seg, segments_index, 20, type_record=True)\n",
    "\n",
    "        # Write the new points with IDs to identify their appropriate segments to file.\n",
    "        print(\"Output {} point-based features to {}\".format(len(features), jsonfile))\n",
    "        print(\"This will take some time.\")\n",
    "        with open(jsonfile, 'w') as f:\n",
    "            json.dump([r.properties for r in features], f)\n",
    "\n",
    "    else:\n",
    "        features = util.read_records(jsonfile, verbose, None)\n",
    "        print(\"Read {} point-based features from file\".format(len(features)))\n",
    "\n",
    "    # For each feature, find it it is near any segments\n",
    "    # If it is, make an entry within matches for that segment, and begin incrementing the feat_type\n",
    "    # That means every time a point is added to a segment, it will increment feat_type\n",
    "    matches = {}\n",
    "    for feature in features:\n",
    "        near = feature.near_id\n",
    "        feat_type = feature.properties['feature']\n",
    "\n",
    "        if near:\n",
    "            if str(near) not in matches:\n",
    "                matches[str(near)] = {}\n",
    "            if feat_type not in matches[str(near)]:\n",
    "                matches[str(near)][feat_type] = 0\n",
    "            matches[str(near)][feat_type] += 1\n",
    "\n",
    "    # Having correlated point data to segments, now go through each intersection and add point data\n",
    "    # Add the point data to all intersections that are combined together\n",
    "    for i, inter in enumerate(inters):\n",
    "        if str(inter['properties']['id']) in list(matches.keys()):\n",
    "            matched_features = matches[str(inter['properties']['id'])]\n",
    "\n",
    "            for prop in inter['properties']['data']:\n",
    "                for feat in matched_features:\n",
    "                    prop[feat] = matched_features[feat]\n",
    "\n",
    "    # Having correlated point data to segments, now go through each non-intersection and add point data\n",
    "    for i, non_inter in enumerate(non_inters):\n",
    "        if str(non_inter['properties']['id']) in list(matches.keys()):\n",
    "            matched_features = matches[non_inter['properties']['id']]\n",
    "\n",
    "            n = copy.deepcopy(non_inter)\n",
    "\n",
    "            for feat in matched_features:\n",
    "                n['properties'][feat] = matched_features[feat]\n",
    "\n",
    "            non_inters[i] = n\n",
    "\n",
    "    return non_inters, inters\n",
    "\n",
    "def write_segments(non_inters, inters, MAP_FP, PROCESSED_DIR):\n",
    "    # Writes out non_inters as non_inters_segments.geojson\n",
    "    # Writes out IDs and properties of intersection as inters_data.json\n",
    "    # Writes out intersections without peroperties for visualisation as inters_segments.geojson\n",
    "    # Writes out combined intersection and non-intersection with all properties as inter_and_non_int.geojson\n",
    "\n",
    "    # Store non-intersection segments\n",
    "    # Project back into 4326 for storage\n",
    "    non_inters = util.prepare_geojson(non_inters)\n",
    "    with open(os.path.join(MAP_FP, 'non_inters_segments.geojson'), 'w') as outfile:\n",
    "        geojson.dump(non_inters, outfile)\n",
    "\n",
    "    # Get just the properties for the intersections\n",
    "    # Store all the intersections in json form like {ID : Properties}\n",
    "    # Save out to inter_data.json\n",
    "\n",
    "    # ============== INTER_DATA USED IN AGGREGATION =============\n",
    "    # inter_data = {str(x['properties']['id']): x['properties']['data'] for x in inters}\n",
    "    inters_data = util.prepare_geojson(inters)\n",
    "    with open(os.path.join(MAP_FP, 'inters_segments.geojson'), 'w') as outfile:\n",
    "        geojson.dump(inters_data, outfile)\n",
    "\n",
    "    # for x in inter_data_prep:\n",
    "    #     for y in x['properties']['data']:\n",
    "    #         y['geometry'] = x['geometry']\n",
    "    #         y['type'] = x['type']\n",
    "    #     inter_data[str(x['properties']['id'])] = x['properties']['data']\n",
    "\n",
    "    # ============== COMBINE INTER AND NON_INTER GEOJSONS ============\n",
    "    # Store the combined segments with all properties\n",
    "    # This line was originally here but I believe bug: segments = non_inters['features'] + int_w_ids['features']\n",
    "    # Instead, change inter_data to geojson then append\n",
    "    segments = non_inters['features'] + inter_data_prep['features']\n",
    "    segments_collection = geojson.FeatureCollection(segments)\n",
    "    with open(os.path.join(MAP_FP, 'inter_and_non_int.geojson'), 'w') as outfile:\n",
    "        geojson.dump(segments_collection, outfile)\n",
    "\n",
    "    # ============= SAVE OUT INTERS_SEGMENTS for VIS ===============\n",
    "    # Store the individual intersections without properties, since QGIS appears\n",
    "    # to have trouble with dicts of dicts, and viewing maps can be helpful\n",
    "    int_w_ids = [{\n",
    "        'geometry': x['geometry'],\n",
    "        'properties': {\n",
    "            'id': x['properties']['id'],\n",
    "            'display_name': x['properties']['display_name'] if 'display_name' in x['properties'] else '',\n",
    "            'center_x': x['properties']['center_x'] if 'center_x' in x['properties'] else '',\n",
    "            'center_y': x['properties']['center_y'] if 'center_y' in x['properties'] else ''\n",
    "        }\n",
    "    } for x in inters]\n",
    "    int_w_ids = util.prepare_geojson(int_w_ids)\n",
    "    \n",
    "    with open(os.path.join(MAP_FP, 'inters_segments.geojson'), 'w') as outfile:\n",
    "        geojson.dump(int_w_ids, outfile)\n",
    "\n",
    "\n",
    "\n",
    "def get_intersection_name(inter_segments):\n",
    "    \"\"\"\n",
    "    Get an intersection name from a set of intersection segment names\n",
    "    Args:\n",
    "        inter_segments - a list of properties\n",
    "    Returns:\n",
    "        intersection name - a string, e.g. First St and Second St\n",
    "    \"\"\"\n",
    "\n",
    "    streets = []\n",
    "    # Some open street maps segments have more than one name in them\n",
    "    for street in [x['name'] if 'name' in x.keys() else None\n",
    "                   for x in inter_segments]:\n",
    "        if street:\n",
    "            if '[' in street:\n",
    "                streets.extend(re.sub(\"['\\[\\]]\", '', street).split(', '))\n",
    "            else:\n",
    "                streets.append(street)\n",
    "    streets = sorted(list(set(streets)))\n",
    "\n",
    "    name = ''\n",
    "    if not streets:\n",
    "        return name\n",
    "    if len(streets) == 2:\n",
    "        name = streets[0] + \" and \" + streets[1]\n",
    "    else:\n",
    "        name = streets[0] + \" near \"\n",
    "        name += ', '.join(streets[1:-1]) + ' and ' + streets[-1]\n",
    "\n",
    "    return name\n",
    "\n",
    "\n",
    "def get_non_intersection_name(non_inter_segment, inters_by_id):\n",
    "    \"\"\"\n",
    "    Get non-intersection segment names. Mostly in the form:\n",
    "    X Street between Y Street and Z Street, but sometimes the\n",
    "    intersection has streets with two different names, in which case\n",
    "    it will be X Street between Y Street/Z Street and A Street,\n",
    "    or it's a dead end, in which case it will be X Street from Y Street\n",
    "    Args:\n",
    "        non_inter_segment - a geojson non-intersection segment\n",
    "        inters_by_id - a dict with OSM node IDs as keys\n",
    "    Returns:\n",
    "        The display name string\n",
    "    \"\"\"\n",
    "\n",
    "    properties = non_inter_segment['properties']\n",
    "\n",
    "    if 'name' not in properties or not properties['name']:\n",
    "        return ''\n",
    "\n",
    "    segment_street = properties['name']\n",
    "    from_streets = None\n",
    "    to_streets = None\n",
    "    if properties['from'] in inters_by_id and inters_by_id[properties['from']]:\n",
    "        from_street = inters_by_id[properties['from']]\n",
    "        from_streets = from_street.split(', ')\n",
    "\n",
    "        # Remove any street that's part of the named street sections\n",
    "        if segment_street in from_streets:\n",
    "            from_streets.remove(segment_street)\n",
    "    if properties['to'] in inters_by_id and inters_by_id[properties['to']]:\n",
    "        to_street = inters_by_id[properties['to']]\n",
    "        to_streets = to_street.split(', ')\n",
    "\n",
    "        # Remove any street that's part of the named street sections\n",
    "        if segment_street in to_streets:\n",
    "            to_streets.remove(segment_street)\n",
    "\n",
    "    if not from_streets and not to_streets:\n",
    "        return segment_street\n",
    "\n",
    "    from_street = None\n",
    "    if from_streets:\n",
    "        from_street = '/'.join(from_streets)\n",
    "    to_street = None\n",
    "    if to_streets:\n",
    "        to_street = '/'.join(to_streets)\n",
    "\n",
    "    if not to_streets:\n",
    "        return segment_street + ' from ' + from_street\n",
    "    if not from_streets:\n",
    "        return segment_street + ' from ' + to_street\n",
    "\n",
    "    return segment_street + ' between ' + from_street + \\\n",
    "        ' and ' + to_street\n",
    "\n",
    "    return segment_street\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take in osm_elements.geojson and output non-inters and inters\n",
    "# Non-inters are all the non-intersection segments from the data.\n",
    "# Inters are all the intersections that have been formed by taking union of overlapping segments near ints.\n",
    "# In both cases, the segments have had appropriate names attached\n",
    "# Non_inters has properties: id [= '99' + i], inter [= 0], display_name, center_y, center_x as well as normal way properties\n",
    "# Inters has properties: id [i], data, display_name, center_x, center_y\n",
    "elements = os.path.join(MAP_DIR, 'osm_elements.geojson')\n",
    "non_inters, inters = create_segments_from_json(elements, MAP_DIR)  # Returns non_int_w_ids, union_inter\n",
    "\n",
    "# Load in the feature file.\n",
    "# Currently this is populated with extra features from the complete nodes shp file.\n",
    "# It has populated those nodes that correspond to traffic lights / crosswalks within the complete nodes file.\n",
    "feats_file = os.path.join(MAP_DIR, 'features.geojson')\n",
    "additional_feats_file = os.path.join(MAP_DIR, 'additional_features.geojson')\n",
    "\n",
    "if not os.path.exists(feats_file):\n",
    "    feats_file = None\n",
    "if not os.path.exists(additional_feats_file):\n",
    "    additional_feats_file = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add point based features [e.g. from features.geojson] and associate them with each segment according to proximity\n",
    "# Returns non_inters and inters, but with the point based features snapped to the appropriate segments\n",
    "# Writes out points_joined.json\n",
    "if feats_file or additional_feats_file:\n",
    "    jsonfile = os.path.join(DATA_DIR, 'processed', 'points_joined.json')\n",
    "    non_inters, inters = add_point_based_features(non_inters, inters, jsonfile, feats_filename=feats_file, additional_feats_filename=None, forceupdate=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out the segments which have been categorised into non-intersection or intersection\n",
    "# And have also had point data attached to them.\n",
    "# Writes out non_inters as non_inters_segments.geojson\n",
    "# Writes out IDs and properties of intersection as inters_data.json\n",
    "# Writes out intersections without properties for visualisation as inters_segments.geojson\n",
    "# Writes out combined intersection and non-intersection with all properties as inter_and_non_int.geojson\n",
    "write_segments(non_inters, inters, MAP_DIR, PROCESSED_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writes out non_inters as non_inters_segments.geojson\n",
    "# Writes out IDs and properties of intersection as inters_data.json\n",
    "# Writes out intersections without peroperties for visualisation as inters_segments.geojson\n",
    "# Writes out combined intersection and non-intersection with all properties as inter_and_non_int.geojson\n",
    "\n",
    "# Store non-intersection segments\n",
    "# Project back into 4326 for storage\n",
    "non_inters = util.prepare_geojson(non_inters)\n",
    "with open(os.path.join(MAP_FP, 'non_inters_segments.geojson'), 'w') as outfile:\n",
    "    geojson.dump(non_inters, outfile)\n",
    "\n",
    "# Get just the properties for the intersections\n",
    "# Store all the intersections in json form like {ID : Properties}\n",
    "# Save out to inter_data.json\n",
    "\n",
    "# ============== INTER_DATA USED IN AGGREGATION =============\n",
    "# inter_data = {str(x['properties']['id']): x['properties']['data'] for x in inters}\n",
    "inters_data = util.prepare_geojson(inters)\n",
    "with open(os.path.join(MAP_FP, 'inters_segments.geojson'), 'w') as outfile:\n",
    "    geojson.dump(inters_data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in inter_data_prep:\n",
    "#     for y in x['properties']['data']:\n",
    "#         y['geometry'] = x['geometry']\n",
    "#         y['type'] = x['type']\n",
    "#     inter_data[str(x['properties']['id'])] = x['properties']['data']\n",
    "\n",
    "# ============== COMBINE INTER AND NON_INTER GEOJSONS ============\n",
    "# Store the combined segments with all properties\n",
    "# This line was originally here but I believe bug: segments = non_inters['features'] + int_w_ids['features']\n",
    "# Instead, change inter_data to geojson then append\n",
    "segments = non_inters['features'] + inter_data_prep['features']\n",
    "segments_collection = geojson.FeatureCollection(segments)\n",
    "with open(os.path.join(MAP_FP, 'inter_and_non_int.geojson'), 'w') as outfile:\n",
    "    geojson.dump(segments_collection, outfile)\n",
    "\n",
    "# ============= SAVE OUT INTERS_SEGMENTS for VIS ===============\n",
    "# Store the individual intersections without properties, since QGIS appears\n",
    "# to have trouble with dicts of dicts, and viewing maps can be helpful\n",
    "int_w_ids = [{\n",
    "    'geometry': x['geometry'],\n",
    "    'properties': {\n",
    "        'id': x['properties']['id'],\n",
    "        'display_name': x['properties']['display_name'] if 'display_name' in x['properties'] else '',\n",
    "        'center_x': x['properties']['center_x'] if 'center_x' in x['properties'] else '',\n",
    "        'center_y': x['properties']['center_y'] if 'center_y' in x['properties'] else ''\n",
    "    }\n",
    "} for x in inters]\n",
    "int_w_ids = util.prepare_geojson(int_w_ids)\n",
    "\n",
    "with open(os.path.join(MAP_FP, 'int_w_ids.geojson'), 'w') as outfile:\n",
    "    geojson.dump(int_w_ids, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(MAP_FP, 'inters_segments.geojson'), 'r') as fp:\n",
    "    inters = geojson.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inters[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join crash segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point, shape, mapping, MultiLineString, LineString\n",
    "import pyproj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snap_records(combined_seg, segments_index, infile, record_type, verbose=True, startyear=None, endyear=None):\n",
    "\n",
    "    print(\"Within join_segments.snap_records\")\n",
    "    print(\"Reading {} data...\".format(record_type))\n",
    "    records = util.read_records(infile, record_type, verbose, startyear, endyear)\n",
    "\n",
    "    # Find nearest crashes - 30 tolerance\n",
    "    # Adds the 'near_id' to our records so that we can correlate with segments\n",
    "    print(\"Snapping \" + record_type + \" records to segments. Tolerance of 30m\")\n",
    "    util.find_nearest(records, combined_seg, segments_index, 30, type_record=True)\n",
    "\n",
    "    jsonfile = os.path.join(PROCESSED_DIR, record_type + '_joined.json')\n",
    "\n",
    "    print(\"output \" + record_type + \" data to \" + jsonfile)\n",
    "    with open(jsonfile, 'w') as f:\n",
    "        json.dump([r.properties for r in records], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest(records, segments, segments_index, tolerance, type_record=False):\n",
    "    \"\"\" Finds nearest segment to records\n",
    "    tolerance : max units distance from record point to consider\n",
    "\n",
    "    Adds near_id to records.\n",
    "    Near_id is the db_segment_id, which comes from segments[segment_id][1]['id']\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Using tolerance {}\".format(tolerance))\n",
    "\n",
    "    for record in records:\n",
    "\n",
    "        # We are in process of transition to using Record class\n",
    "        # but haven't converted it everywhere, so until we do, need\n",
    "        # to look at whether the records are of type record or not\n",
    "        record_point = None\n",
    "        if type_record:\n",
    "            record_point = record.point\n",
    "        else:\n",
    "            record_point = record['point']\n",
    "\n",
    "        record_buffer_bounds = record_point.buffer(tolerance).bounds\n",
    "        nearby_segments = segments_index.intersection(record_buffer_bounds)\n",
    "\n",
    "        # Populate a new list 'segment_id_with_distance'\n",
    "        # Include in it all the nearby segments to each record\n",
    "        # Add the ID and the distance of this segment\n",
    "        segment_id_with_distance = [\n",
    "            # Get db index and distance to point\n",
    "            (\n",
    "                segments[segment_id][1]['id'],\n",
    "                segments[segment_id][0].distance(record_point)\n",
    "            )\n",
    "            for segment_id in nearby_segments\n",
    "        ]\n",
    "\n",
    "        # From 'segment_id_with_distance', find the entry with the smallest distance\n",
    "        # Populate record['properties']['near_id'] with the segment_id that is closest\n",
    "        if len(segment_id_with_distance):\n",
    "            nearest = min(segment_id_with_distance, key=lambda tup: tup[1])\n",
    "            db_segment_id = nearest[0]\n",
    "            # Add db_segment_id to record\n",
    "            if type_record:\n",
    "                record.near_id = db_segment_id\n",
    "            else:\n",
    "                record['properties']['near_id'] = db_segment_id\n",
    "        # If no segment matched, populate key = ''\n",
    "        else:\n",
    "            if type_record:\n",
    "                record.near_id = ''\n",
    "            else:\n",
    "                record['properties']['near_id'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_records(filename, record_type, verbose, startdate=None, enddate=None):\n",
    "    \"\"\"\n",
    "    Reads appropriately formatted json file,\n",
    "    pulls out currently relevant features,\n",
    "    converts latitude and longitude to projection 4326, and turns into\n",
    "    a Crash object\n",
    "    Args:\n",
    "        filename - json file\n",
    "        start - optionally give start for date range of crashes\n",
    "        end - optionally give end date after which to exclude crashes\n",
    "    Returns:\n",
    "        A list of Crashes\n",
    "    \"\"\"\n",
    "\n",
    "    if verbose:\n",
    "        print('Within util.read_records with the following variables')\n",
    "        print('Filename:', filename)\n",
    "        print('Record_type:', record_type)\n",
    "        print('Startdate:', startdate)\n",
    "        print('Enddate:', enddate)\n",
    "\n",
    "    records = []\n",
    "    items = json.load(open(filename))\n",
    "\n",
    "    if not items:\n",
    "        return []\n",
    "\n",
    "    for item in items:\n",
    "        record = None\n",
    "        event = items[item]\n",
    "        event['ACCIDENT_NO'] = item\n",
    "\n",
    "        if record_type == 'crash':\n",
    "            record = Crash(event)\n",
    "        else:\n",
    "            record = Record(event)\n",
    "\n",
    "        records.append(record)\n",
    "\n",
    "    print(\"Read in data from {} records\".format(len(records)))\n",
    "\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reproject_records(records, inproj='epsg:4326', outproj='epsg:3857'):\n",
    "    \"\"\"\n",
    "    Reprojects a set of records from one projection to another\n",
    "    Records can either be points, line strings, or multiline strings\n",
    "    Args:\n",
    "        records - list of records to reproject\n",
    "        inproj - defaults to 4326\n",
    "        outproj - defaults to 3857\n",
    "    Returns:\n",
    "        list of reprojected records\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    inproj = pyproj.Proj(init=inproj)\n",
    "    outproj = pyproj.Proj(init=outproj)\n",
    "\n",
    "    for record in records:\n",
    "\n",
    "        coords = record['geometry']['coordinates']\n",
    "        if record['geometry']['type'] == 'Point':\n",
    "            re_point = pyproj.transform(inproj, outproj, coords[0], coords[1])\n",
    "            point = Point(re_point)\n",
    "            results.append({'geometry': point,\n",
    "                            'properties': record['properties']})\n",
    "        elif record['geometry']['type'] == 'MultiLineString':\n",
    "            new_coords = []\n",
    "            for segment in coords:\n",
    "                new_segment = []\n",
    "                for coord in segment:\n",
    "                    new_segment.append(pyproj.transform(\n",
    "                        inproj, outproj, coord[0], coord[1]))\n",
    "                new_coords.append(new_segment)\n",
    "\n",
    "            results.append({'geometry': MultiLineString(new_coords),\n",
    "                            'properties': record['properties']})\n",
    "        elif record['geometry']['type'] == 'LineString':\n",
    "            new_coords = []\n",
    "            for coord in coords:\n",
    "                new_coords.append(\n",
    "                    pyproj.transform(inproj, outproj, coord[0], coord[1])\n",
    "                )\n",
    "            results.append({'geometry': LineString(new_coords),\n",
    "                            'properties': record['properties']})\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_segments(segments, geojson=True, segment=False):\n",
    "    \"\"\"\n",
    "    Reads a list of segments in geojson format, and makes\n",
    "    a spatial index for lookup\n",
    "    Args:\n",
    "        list of segments\n",
    "        geojson - whether or not the list of tuples are in geojson format\n",
    "            (the other option is shapely shapes) defaults to True\n",
    "    Returns:\n",
    "        segments (in shapely format), and segments_index\n",
    "    \"\"\"\n",
    "\n",
    "    combined_seg = segments\n",
    "    if segment:\n",
    "        combined_seg = [(x.geometry, x.properties) for x in segments]\n",
    "    elif geojson:\n",
    "        # Read in segments and turn them into shape, propery tuples\n",
    "        combined_seg = [(shape(x['geometry']), x['properties']) for x in\n",
    "                        segments]\n",
    "    # Create spatial index for quick lookup\n",
    "    segments_index = rtree.index.Index()\n",
    "    for idx, element in enumerate(combined_seg):\n",
    "        segments_index.insert(idx, element[0].bounds)\n",
    "\n",
    "    # print('Within util.index_segments creating segments and segments_index')\n",
    "    # print('Segments_index is our spatial index:', segments_index)\n",
    "    # print('Combined_seg is our geojson:', combined_seg)\n",
    "\n",
    "    return combined_seg, segments_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Reads in inters_segments.geojson and non_inters_segments.geojson\n",
    "    # Returns all the segments, along with an index that can be used for spatial lookups\n",
    "    combined_seg, segments_index = util.read_segments(dirname=MAP_DIR)\n",
    "\n",
    "    # Creates crashes_joined.json, which gives crash data a 'near_id', which maps it to the appropriate OSM segment [not the OSM ID, it uses the ID generated for combined_seg]\n",
    "    infile = os.path.join(CRASH_DIR, 'crashes.json')\n",
    "    snap_records(combined_seg, segments_index, infile, record_type='crash', startyear=None, endyear=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = MAP_FP\n",
    "get_inter = True\n",
    "get_non_inter = True\n",
    "\n",
    "inter = []\n",
    "non_inter = []\n",
    "\n",
    "inter = fiona.open(dirname + '/inters_segments.geojson')\n",
    "inter_reproj = reproject_records([x for x in inter])\n",
    "\n",
    "inter_old = inter\n",
    "inter = inter_reproj\n",
    "\n",
    "inter = [{\n",
    "    'geometry': mapping(x['geometry']),\n",
    "    'properties': x['properties']} for x in inter]\n",
    "\n",
    "non_inter = fiona.open(dirname + '/non_inters_segments.geojson')\n",
    "non_inter = reproject_records([x for x in non_inter])\n",
    "non_inter = [{\n",
    "    'geometry': mapping(x['geometry']),\n",
    "    'properties': x['properties']} for x in non_inter]\n",
    "\n",
    "\n",
    "print(\"Read in {} intersection, {} non-intersection segments\".format(len(inter), len(non_inter)))\n",
    "segments_list = list(inter) + list(non_inter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_seg, segments_index = index_segments(segments_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = os.path.join(CRASH_DIR, 'crashes.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_type = 'crash'\n",
    "verbose=True\n",
    "startyear=None\n",
    "endyear=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Within join_segments.snap_records\")\n",
    "print(\"Reading {} data...\".format(record_type))\n",
    "records = util.read_records(infile, record_type, verbose, startyear, endyear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records[85].near_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "util.find_nearest(records, combined_seg, segments_index, 30, type_record=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records[85].near_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonfile = os.path.join(PROCESSED_DIR, record_type + '_joined.json')\n",
    "\n",
    "print(\"output \" + record_type + \" data to \" + jsonfile)\n",
    "with open(jsonfile, 'w') as f:\n",
    "    json.dump([r.properties for r in records], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = records\n",
    "segments = combined_seg\n",
    "segments_index = segments_index\n",
    "tolerance=30\n",
    "type_record=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find nearest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for record in records:\n",
    "    i+=1\n",
    "    if i == 86:\n",
    "        \n",
    "        # We are in process of transition to using Record class\n",
    "        # but haven't converted it everywhere, so until we do, need\n",
    "        # to look at whether the records are of type record or not\n",
    "        record_point = None\n",
    "        if type_record:\n",
    "            record_point = record.point\n",
    "        else:\n",
    "            record_point = record['point']\n",
    "\n",
    "        record_buffer_bounds = record_point.buffer(tolerance).bounds\n",
    "        nearby_segments = segments_index.intersection(record_buffer_bounds)\n",
    "\n",
    "        # Populate a new list 'segment_id_with_distance'\n",
    "        # Include in it all the nearby segments to each record\n",
    "        # Add the ID and the distance of this segment\n",
    "\n",
    "        segment_id_with_distance = [\n",
    "            (segments[segment_id][1]['id'], segments[segment_id][0].distance(record_point)) \n",
    "            for segment_id in nearby_segments]\n",
    "\n",
    "        # From 'segment_id_with_distance', find the entry with the smallest distance\n",
    "        # Populate record['properties']['near_id'] with the segment_id that is closest\n",
    "        if len(segment_id_with_distance):\n",
    "            nearest = min(segment_id_with_distance, key=lambda tup: tup[1])\n",
    "            db_segment_id = nearest[0]\n",
    "            # Add db_segment_id to record\n",
    "            if type_record:\n",
    "                record.near_id = db_segment_id\n",
    "            else:\n",
    "                record['properties']['near_id'] = db_segment_id\n",
    "        # If no segment matched, populate key = ''\n",
    "        else:\n",
    "            if type_record:\n",
    "                record.near_id = ''\n",
    "            else:\n",
    "                record['properties']['near_id'] = ''\n",
    "        print(record)\n",
    "        print(record.near_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find nearest crashes - 30 tolerance\n",
    "# Adds the 'near_id' to our records so that we can correlate with segments\n",
    "print(\"Snapping \" + record_type + \" records to segments. Tolerance of 30m\")\n",
    "util.find_nearest(records, combined_seg, segments_index, 30, type_record=True)\n",
    "\n",
    "jsonfile = os.path.join(PROCESSED_DIR, record_type + '_joined.json')\n",
    "\n",
    "print(\"output \" + record_type + \" data to \" + jsonfile)\n",
    "with open(jsonfile, 'w') as f:\n",
    "    json.dump([r.properties for r in records], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_seg[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Canon dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import csv\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "from util import read_geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_json(json, num=1):\n",
    "    i = 0\n",
    "    for j in json:\n",
    "        if i in range(num):\n",
    "            print(j)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crash_data(crash_joined_path):\n",
    "\n",
    "    # Load in crash data\n",
    "    with open(crash_joined_path, 'r') as f:\n",
    "        crashes = json.load(f)\n",
    "    crash = pd.DataFrame(crashes)\n",
    "\n",
    "    print(\"Number of read in crashes is:\", len(crash))\n",
    "    # DF cleaning\n",
    "    # First, drop accidents that don't have a near_id mapping\n",
    "    # Turn 'near_id' to a string, as it may be mixed datatype here\n",
    "    # Sort by accident date for data transparency\n",
    "    # Finally, when crash was turned to JSON the DATE_TIME was turned to UNIX. Turn it back.\n",
    "    crash['near_id'].replace('', np.nan, inplace=True)\n",
    "    crash.dropna(inplace=True)\n",
    "    crash['near_id'] = crash['near_id'].astype(str)\n",
    "    crash['DATE_TIME'] = pd.to_datetime(crash['DATE_TIME'], unit='ms')\n",
    "    crash.sort_values('DATE_TIME', inplace=True)\n",
    "\n",
    "    print(\"Number of mapped crashes [i.e. given a nearID] after cleaning is:\", len(crash))\n",
    "    return crash\n",
    "\n",
    "\n",
    "def unnest(df, col, reset_index=False):\n",
    "\n",
    "    col_flat = pd.DataFrame([[i, x]\n",
    "                             for i, y in df[col].apply(list).iteritems()\n",
    "                             for x in y], columns=['I', col])\n",
    "    col_flat = col_flat.set_index('I')\n",
    "    df = df.drop(col, 1)\n",
    "    df = df.merge(col_flat, left_index=True, right_index=True)\n",
    "    if reset_index:\n",
    "        df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def aggregate(PROCESSED_DIR, keep_feat=None, agg='max'):\n",
    "    \"\"\"\n",
    "    Revised version of aggregate_inter_noninter\n",
    "    \"\"\"\n",
    "\n",
    "    # Read in the combined road feature dataset parameters\n",
    "    inters_fp = os.path.join(PROCESSED_DIR, 'maps', 'inters_segments.geojson')\n",
    "    non_inters_fp = os.path.join(PROCESSED_DIR, 'maps', 'non_inters_segments.geojson')\n",
    "\n",
    "    non_inters = read_geojson(non_inters_fp)\n",
    "    inters = read_geojson(inters_fp)\n",
    "\n",
    "    with open(non_inters_fp, 'r') as fp:\n",
    "        non_inters = geojson.load(fp)\n",
    "\n",
    "    with open(inters_fp, 'r') as fp:\n",
    "        inters = geojson.load(fp)\n",
    "\n",
    "    inters_df = pd.DataFrame(inters['features'], columns=['geometry', 'properties'])\n",
    "    non_inters_df = pd.DataFrame(non_inters['features'], columns=['geometry', 'properties'])\n",
    "\n",
    "    properties_flattened = pd.io.json.json_normalize(inters_df['properties'])\n",
    "    inters_df = pd.concat([inters_df, properties_flattened], axis=1)\n",
    "    inters_df = unnest(inters_df, 'data')\n",
    "    inters_df.drop(columns=\"display_name\", inplace=True)\n",
    "    inters_df.rename(columns={'id': 'inter_id'}, inplace=True)\n",
    "\n",
    "    data_flattened = pd.io.json.json_normalize(inters_df['data'])\n",
    "    inters_df.reset_index(drop=True, inplace=True)\n",
    "    inters_df = pd.concat([inters_df, data_flattened], axis=1)\n",
    "\n",
    "    geometry_flattened = pd.io.json.json_normalize(inters_df['geometry'])\n",
    "    geometry_flattened.rename(columns={'type': 'geometry.type'}, inplace=True)\n",
    "    inters_df = pd.concat([inters_df, geometry_flattened], axis=1)\n",
    "\n",
    "    inters_df.drop(columns=['properties', 'data', 'geometry'], inplace=True)\n",
    "    inters_df.set_index('inter_id', inplace=True)\n",
    "\n",
    "    properties_flattened = pd.io.json.json_normalize(non_inters_df['properties'])\n",
    "    non_inters_df = pd.concat([non_inters_df, properties_flattened], axis=1)\n",
    "\n",
    "    geometry_flattened = pd.io.json.json_normalize(non_inters_df['geometry'])\n",
    "    geometry_flattened.rename(columns={'type': 'geometry.type'}, inplace=True)\n",
    "    non_inters_df = pd.concat([non_inters_df, geometry_flattened], axis=1)\n",
    "\n",
    "    non_inters_df.drop(columns=[\"properties\", \"inter\", \"geometry\"], inplace=True)\n",
    "    non_inters_df['intersection_segments'] = 0\n",
    "    non_inters_df['intersection'] = 0\n",
    "\n",
    "    combined = pd.concat([inters_df, non_inters_df], sort=True)\n",
    "    combined['type'] = 'feature'\n",
    "    combined_geojson = df_to_geojson(combined, properties=['intersection'])\n",
    "    with open(os.path.join(MAP_FP, 'combined.geojson'), 'w') as outfile:\n",
    "        geojson.dump(combined_geojson, outfile)\n",
    "\n",
    "    # Get the feature list as the intersection of the passed in feature list and the combined dataframe column names\n",
    "    # If keep_feat is not passed in, just take the feature list as the full feature list\n",
    "    if keep_feat:\n",
    "        print(\"Trimming the features from OSM to match our keep_feat list\")\n",
    "        OSM_feat = list(set.intersection(set(combined), set(keep_feat)))\n",
    "    else:\n",
    "        print(\"Keeping all features from OSM\")\n",
    "        OSM_feat = list(set(combined))\n",
    "\n",
    "    # Since there are multiple segments per intersection, we must merge them somehow\n",
    "    # We aggregate by taking the maximum of all incoming intersections\n",
    "    aggregated = getattr(combined[OSM_feat].groupby(combined.index), agg)\n",
    "\n",
    "    # Return aggregation and adjacency info (orig_id)\n",
    "    print('About to aggregate inter / non-inter dataframes. This takes quite some time')\n",
    "    print(\"About to aggregate data. This takes quite some time. Road features that are included: \", ', '.join(OSM_feat))\n",
    "    aggregated = aggregated()\n",
    "\n",
    "    # Many NA values, as the intersection / non-intersection data was dealt with differently\n",
    "    aggregated = aggregated.fillna(0)\n",
    "\n",
    "    return aggregated, OSM_feat\n",
    "\n",
    "\n",
    "def aggregate_inter_noninter(PROCESSED_DIR, keep_feat=None, agg='max'):\n",
    "    \"\"\"\n",
    "    Makes road feature df by combining intersections + non-intersecions\n",
    "\n",
    "    In particular:\n",
    "    - selects features using the input 'feats'\n",
    "    - aggregates intersections by taking the maximum for each of the inputs\n",
    "    - Returns the concat'd and aggregated data as well as the original IDs of all the road segments\n",
    "\n",
    "    IMPORTANT: if the aggregation type changes, need to also update how aggregation is calculated in src/data/add_map.py\n",
    "    \"\"\"\n",
    "\n",
    "    print('Within make_canon_dataset.aggregate_inter_noninter')\n",
    "\n",
    "    # Read in the combined road feature dataset parameters\n",
    "    inters_fp = os.path.join(PROCESSED_DIR, 'inters_data.json')\n",
    "    non_inters_fp = os.path.join(PROCESSED_DIR, 'maps', 'non_inters_segments.geojson')\n",
    "\n",
    "    # Create combined road feature dataset\n",
    "    # Aggregated contains concat'd intersections / non-intersections, as well as filtering chosen features\n",
    "    # Aggregates intersection features by taking the maximum of all incoming roads.\n",
    "    # Read in inters data (json), turn into df with inter index\n",
    "    df_index = []\n",
    "    df_records = []\n",
    "    print(\"In make_canon_dataset.aggregate_inter_noninter\")\n",
    "    print(\"Reading in:\", inters_fp)\n",
    "\n",
    "    with open(inters_fp, 'r') as f:\n",
    "        inters = json.load(f)\n",
    "\n",
    "        # Append each index to dataframe\n",
    "        # Each intersection has more than one segment\n",
    "        # Add each segment's properties to df_records\n",
    "        for idx, lines in inters.items():\n",
    "            df_records.extend(lines)\n",
    "            df_index.extend([idx] * len(lines))\n",
    "\n",
    "    # Having read in all our data, turn it into a df with one index per intersection\n",
    "    # Keep in mind that intersections can have multiple segments, meaning that there will be multiple entries\n",
    "    inters_df = pd.DataFrame(df_records, index=df_index)\n",
    "\n",
    "    # Read in non_inters data\n",
    "    print(\"Reading in:\", non_inters_fp)\n",
    "    non_inters = read_geojson(non_inters_fp)\n",
    "    non_inters_df = pd.DataFrame([x[1] for x in non_inters])\n",
    "    non_inters_df.set_index('id', inplace=True)\n",
    "\n",
    "    # Combining intersections and non-intersections\n",
    "    combined = pd.concat([inters_df, non_inters_df], sort=True)\n",
    "\n",
    "    # Get the feature list as the intersection of the passed in feature list and the combined dataframe column names\n",
    "    # If keep_feat is not passed in, just take the feature list as the full feature list\n",
    "    if keep_feat:\n",
    "        print(\"Trimming the features from OSM to match our keep_feat list\")\n",
    "        OSM_feat = list(set.intersection(set(combined), set(keep_feat)))\n",
    "    else:\n",
    "        print(\"Keeping all features from OSM\")\n",
    "        OSM_feat = list(set(combined))\n",
    "\n",
    "    # Since there are multiple segments per intersection, we must merge them somehow\n",
    "    # We aggregate by taking the maximum of all incoming intersections\n",
    "    aggregated = getattr(combined[OSM_feat].groupby(combined.index), agg)\n",
    "\n",
    "    # Return aggregation and adjacency info (orig_id)\n",
    "    print('About to aggregate inter / non-inter dataframes. This takes quite some time')\n",
    "    print(\"About to aggregate data. This takes quite some time. Road features that are included: \", ', '.join(OSM_feat))\n",
    "    aggregated = aggregated()\n",
    "\n",
    "    # Many NA values, as the intersection / non-intersection data was dealt with differently\n",
    "    aggregated = aggregated.fillna(0)\n",
    "\n",
    "    return aggregated, OSM_feat\n",
    "\n",
    "\n",
    "\n",
    "def make_negative_data(crash, roads):\n",
    "    # If roads is not false, we are providing a real-world representation of road geometries from which we can sample, rather than\n",
    "    # having to generate them in a dodgy method.\n",
    "\n",
    "    # A big problem here is in the real-world, there is 1 data-point for every hour, for every road that doesn't see a crash\n",
    "    # This is obviously impossible to actually use, as it would be a huge number of roads\n",
    "    # Instead, we have to try to sample what the actual distribution looks like.\n",
    "    # In doing so, we can uniformly sample some features, while for other features we have to attempt to get accurate real-world distributions.\n",
    "\n",
    "    # Uniform distribution of date times that start at first crash and ends at last crash\n",
    "    # Uniform distribution of segments drawn from the crash data\n",
    "    # Attach date times to segments\n",
    "    # Get weather data that is relevant to these times / segments, or at least to the hour in Melbourne on that day\n",
    "\n",
    "    # Begin by getting all the features we can generate uniformly\n",
    "    # Date, day of week, hour, month, direction. light_cond [dodgy]\n",
    "    # This may seem like it is biasing the data, but I assure you it isn't [or at least, is only VERY slightly biasing].\n",
    "    # Think real hard and you'll work out why.\n",
    "    max_date = crash['DATE_TIME'].max()\n",
    "    min_date = crash['DATE_TIME'].min()\n",
    "    days_difference = (max_date - min_date).days\n",
    "\n",
    "    num_pos_data = len(crash)\n",
    "    num_neg_data_day = int(np.ceil(num_pos_data / days_difference))\n",
    "\n",
    "    dates_in_range = pd.date_range(min_date, max_date).tolist()\n",
    "    uniform_dates_times = pd.DataFrame({'DATE_TIME': dates_in_range * num_neg_data_day})\n",
    "    uniform_dates_times['DAY_OF_WEEK'] = uniform_dates_times['DATE_TIME'].dt.weekday\n",
    "    uniform_dates_times['MONTH'] = uniform_dates_times['DATE_TIME'].dt.month\n",
    "\n",
    "    # Generate the hour uniformly and append it to uniform_dates_times['HOUR']\n",
    "    uniform_dates_times['HOUR'] = np.random.choice(range(23), len(uniform_dates_times))\n",
    "\n",
    "    # Use a true sampling of roads if possible.\n",
    "    # Otherwise generate the direction uniformly and append it to uniform_dates_times['direction']\n",
    "    sampled_roads = roads.sample(len(uniform_dates_times), replace=True)\n",
    "\n",
    "    # Trim down data-frames so that they are both the same length\n",
    "    len_dates = len(uniform_dates_times)\n",
    "    len_roads = len(sampled_roads)\n",
    "    min_len = min(len_dates, len_roads)\n",
    "\n",
    "    uniform_dates_times.drop(uniform_dates_times.index[min_len:], inplace=True)\n",
    "    sampled_roads.drop(sampled_roads.index[min_len:], inplace=True)\n",
    "\n",
    "    # Ensure they are they same length\n",
    "    if len(uniform_dates_times) != len(sampled_roads):\n",
    "        print('Within make_canon_dataset, but did not properly trim our negative data. Exiting.')\n",
    "        sys.exit()\n",
    "    else:\n",
    "        print('Have trimmed negative dataset. Now contains {} results compared to the {} results of positive data'.format(len(uniform_dates_times), len(crash)))\n",
    "\n",
    "    # Now join the dates and segment features\n",
    "    # Will reset index on both so that they share the same index values to merge on\n",
    "    # https://stackoverflow.com/questions/29576430/shuffle-dataframe-rows\n",
    "    sampled_roads = sampled_roads.reset_index(drop=True)\n",
    "    uniform_dates_times = uniform_dates_times.reset_index(drop=True)\n",
    "    dates_and_roads = pd.concat([uniform_dates_times, sampled_roads], axis=1).sort_values('DATE_TIME')\n",
    "\n",
    "    dates_and_roads['ACCIDENT_NO'] = ['Neg' + str(x) for x in range(len(dates_and_roads))]\n",
    "    dates_and_roads['TARGET'] = 0\n",
    "\n",
    "    return dates_and_roads\n",
    "\n",
    "\n",
    "def add_accident_count(df, PROCESSED_DIR, forceupdate=False):\n",
    "    \"\"\"\n",
    "    df = crashes dataframe combined with OSM data\n",
    "    PROCESSED_DIR = data directory path\n",
    "    \"\"\"\n",
    "\n",
    "    # Add columns regarding historical crash counts to each event in crashes.csv\n",
    "    # Currently 7 day, 30 day, 365 day, 1825 day and 3650 day time periods\n",
    "    # This takes a long time to run, so try to load instead from csv file if already exists\n",
    "    accident_count_fp = os.path.join(PROCESSED_DIR, 'crash/accident_count.pk')\n",
    "\n",
    "    if forceupdate or not os.path.exists(accident_count_fp):\n",
    "        print('In make_canon_dataset. About to append accident count by time period to crash data. This takes a long time [2 - 3 hours] and is unoptimised. Fix in future.')\n",
    "        print('Have no built in graceful exit if you wish to leave this, simple cntrl-c. The make_canon_dataset.py is fast up until here regardless.')\n",
    "        accident_count = {}\n",
    "        lenDF = len(df)\n",
    "\n",
    "        for i in range(lenDF):\n",
    "\n",
    "            accident_no = df.iloc[i].ACCIDENT_NO\n",
    "            accident_same_location = df[(df.near_id == df.iloc[i].near_id) & (df.TARGET == 1)]\n",
    "            accident_same_location_date = accident_same_location.DATE_TIME\n",
    "\n",
    "            current_accident_date = df.iloc[i].DATE_TIME\n",
    "\n",
    "            past_7_days = current_accident_date - pd.to_timedelta(\"7day\")\n",
    "            past_30_days = current_accident_date - pd.to_timedelta(\"30day\")\n",
    "            past_365_days = current_accident_date - pd.to_timedelta(\"365day\")\n",
    "            past_1825_days = current_accident_date - pd.to_timedelta(\"1825day\")\n",
    "            past_3650_days = current_accident_date - pd.to_timedelta(\"3650day\")\n",
    "\n",
    "            idx_in_the_past = [accident_same_location_date < current_accident_date]\n",
    "            idx_last_7_days = [accident_same_location_date > past_7_days]\n",
    "            idx_last_30_days = [accident_same_location_date > past_30_days]\n",
    "            idx_last_365_days = [accident_same_location_date > past_365_days]\n",
    "            idx_last_1825_days = [accident_same_location_date > past_1825_days]\n",
    "            idx_last_3650_days = [accident_same_location_date > past_3650_days]\n",
    "\n",
    "            idx_7 = [x & y for (x, y) in zip(idx_in_the_past, idx_last_7_days)]\n",
    "            idx_30 = [x & y for (x, y) in zip(idx_in_the_past, idx_last_30_days)]\n",
    "            idx_365 = [x & y for (x, y) in zip(idx_in_the_past, idx_last_365_days)]\n",
    "            idx_1825 = [x & y for (x, y) in zip(idx_in_the_past, idx_last_1825_days)]\n",
    "            idx_3650 = [x & y for (x, y) in zip(idx_in_the_past, idx_last_3650_days)]\n",
    "\n",
    "            accident_last_7_days = sum(sum((idx_7)))\n",
    "            accident_last_30_days = sum(sum((idx_30)))\n",
    "            accident_last_365_days = sum(sum((idx_365)))\n",
    "            accident_last_1825_days = sum(sum((idx_1825)))\n",
    "            accident_last_3650_days = sum(sum((idx_3650)))\n",
    "\n",
    "            accident_count[accident_no] = [accident_last_7_days, accident_last_30_days, accident_last_365_days, accident_last_1825_days, accident_last_3650_days]\n",
    "\n",
    "            if i % 1000 == 0:\n",
    "                print('{}% done.'.format(i / lenDF * 100))\n",
    "\n",
    "        with open(accident_count_fp, 'wb') as fp:\n",
    "            pickle.dump(accident_count, fp)\n",
    "\n",
    "    # If the file already exists then we can simple read it in.\n",
    "    # Note the list comprehension is due to the strange saving behaviour of previous section, leaves empty rows\n",
    "    else:\n",
    "        with open(accident_count_fp, 'rb') as fp:\n",
    "            print(\"Reading in the accident counts for all segments\")\n",
    "            accident_count = pickle.load(fp)\n",
    "\n",
    "    # Now append the new accident counts to the crashes df\n",
    "    df['LAST_7_DAYS'] = ''\n",
    "    df['LAST_30_DAYS'] = ''\n",
    "    df['LAST_365_DAYS'] = ''\n",
    "    df['LAST_1825_DAYS'] = ''\n",
    "    df['LAST_3650_DAYS'] = ''\n",
    "    LAST_N_DAYS_LIST = ['LAST_7_DAYS', 'LAST_30_DAYS', 'LAST_365_DAYS', 'LAST_1825_DAYS', 'LAST_3650_DAYS']\n",
    "    # df[LAST_N_DAYS_LIST] = accident_count\n",
    "\n",
    "    df_accident_count_path = os.path.join(PROCESSED_DIR, 'crash/df_accident_count.pk')\n",
    "\n",
    "    # Must append the accident count to our df.\n",
    "    # Doing this takes quite some time, so we save it out once complete / load it in if already exists.\n",
    "\n",
    "    # Creating and saving out to pickle\n",
    "    if not os.path.exists(df_accident_count_path) or forceupdate:\n",
    "        print('Appending the historic accident counts to our dataframe. This also takes ~30 minutes.')\n",
    "\n",
    "        i = 0\n",
    "        len_accident_count = len(accident_count)\n",
    "\n",
    "        for key in accident_count.keys():\n",
    "            for idx, last_n_days in enumerate(LAST_N_DAYS_LIST):\n",
    "                df[last_n_days].loc[key] = accident_count[key][idx]\n",
    "            i += 1\n",
    "            if i % 10000 == 0:\n",
    "                print('Appended {}% of results'.format(100 * i / len_accident_count))\n",
    "\n",
    "        with open(df_accident_count_path, 'wb') as fp:\n",
    "            pickle.dump(df, fp)\n",
    "\n",
    "    # Loading in from pickle\n",
    "    else:\n",
    "        print('Loading in df_with_accident_count from pickle')\n",
    "        with open(df_accident_count_path, 'rb') as fp:\n",
    "            df = pickle.load(fp)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OSM_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read in the combined road feature dataset parameters\n",
    "inters_fp = os.path.join(PROCESSED_DIR, 'maps', 'inters_segments.geojson')\n",
    "non_inters_fp = os.path.join(PROCESSED_DIR, 'maps', 'non_inters_segments.geojson')\n",
    "\n",
    "non_inters = read_geojson(non_inters_fp)\n",
    "inters = read_geojson(inters_fp)\n",
    "\n",
    "with open(non_inters_fp, 'r') as fp:\n",
    "    non_inters = geojson.load(fp)\n",
    "\n",
    "with open(inters_fp, 'r') as fp:\n",
    "    inters = geojson.load(fp)\n",
    "    \n",
    "inters_df = pd.DataFrame(inters['features'], columns=['geometry', 'properties'])\n",
    "non_inters_df = pd.DataFrame(non_inters['features'], columns=['geometry', 'properties'])\n",
    "\n",
    "properties_flattened = pd.io.json.json_normalize(inters_df['properties'])\n",
    "inters_df = pd.concat([inters_df, properties_flattened], axis=1)\n",
    "inters_df = unnest(inters_df, 'data')\n",
    "inters_df.drop(columns=\"display_name\", inplace=True)\n",
    "inters_df.rename(columns={'id': 'correlate_id'}, inplace=True)\n",
    "data_flattened = pd.io.json.json_normalize(inters_df['data'])\n",
    "inters_df.reset_index(drop=True, inplace=True)\n",
    "inters_df = pd.concat([inters_df, data_flattened], axis=1)\n",
    "geometry_flattened = pd.io.json.json_normalize(inters_df['geometry'])\n",
    "geometry_flattened.rename(columns={'type': 'geometry.type'}, inplace=True)\n",
    "inters_df = pd.concat([inters_df, geometry_flattened], axis=1)\n",
    "inters_df.drop(columns=['properties', 'data', 'geometry'], inplace=True)\n",
    "\n",
    "properties_flattened = pd.io.json.json_normalize(non_inters_df['properties'])\n",
    "non_inters_df = pd.concat([non_inters_df, properties_flattened], axis=1)\n",
    "non_inters_df.rename(columns={'id':'correlate_id'}, inplace=True)\n",
    "geometry_flattened = pd.io.json.json_normalize(non_inters_df['geometry'])\n",
    "geometry_flattened.rename(columns={'type': 'geometry.type'}, inplace=True)\n",
    "non_inters_df = pd.concat([non_inters_df, geometry_flattened], axis=1)\n",
    "non_inters_df.drop(columns=[\"properties\", \"inter\", \"geometry\"], inplace=True)\n",
    "non_inters_df['intersection_segments'] = 0\n",
    "non_inters_df['intersection'] = 0\n",
    "\n",
    "combined = pd.concat([inters_df, non_inters_df], sort=True)\n",
    "combined['type'] = 'feature'\n",
    "combined.reset_index(inplace=True)\n",
    "combined.correlate_id = combined.correlate_id.astype(str)\n",
    "combined_geojson = df_to_geojson(combined, properties=['intersection'])\n",
    "with open(os.path.join(MAP_FP, 'combined.geojson'), 'w') as outfile:\n",
    "    geojson.dump(combined_geojson, outfile)\n",
    "\n",
    "# Get the feature list as the intersection of the passed in feature list and the combined dataframe column names\n",
    "# If keep_feat is not passed in, just take the feature list as the full feature list\n",
    "if keep_feat:\n",
    "    print(\"Trimming the features from OSM to match our keep_feat list\")\n",
    "    OSM_feat = list(set.intersection(set(combined), set(keep_feat)))\n",
    "else:\n",
    "    print(\"Keeping all features from OSM\")\n",
    "    OSM_feat = list(set(combined))\n",
    "\n",
    "# Since there are multiple segments per intersection, we must merge them somehow\n",
    "# We aggregate by taking the maximum of all incoming intersections\n",
    "aggregated = getattr(combined[OSM_feat].groupby(combined.correlate_id), agg)\n",
    "\n",
    "# Return aggregation and adjacency info (orig_id)\n",
    "print('About to aggregate inter / non-inter dataframes. This takes quite some time')\n",
    "print(\"About to aggregate data. This takes quite some time. Road features that are included: \", ', '.join(OSM_feat))\n",
    "aggregated = aggregated()\n",
    "\n",
    "print('Going to drop any columns that have > 95% as NA. For others fill the NA with \"\" ')\n",
    "perc_na = round(aggregated.isna().sum()/len(aggregated),2)\n",
    "keep = perc_na[perc_na < 0.95].keys()\n",
    "print('Dropped {}'.format(set(list(aggregated))-set(keep)))\n",
    "aggregated = aggregated[keep]\n",
    "\n",
    "# Many NA values, as the intersection / non-intersection data was dealt with differently\n",
    "aggregated = aggregated.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature details\n",
    "cat_feat = config['cat_feat']\n",
    "cont_feat = config['cont_feat']\n",
    "keep_feat = config['keep_feat']\n",
    "\n",
    "# Crash data is read in, sorted by date and any crashes that haven't been mapped to a near_id are dropped\n",
    "crash = get_crash_data(os.path.join(PROCESSED_DIR, 'crash_joined.json'))\n",
    "\n",
    "\n",
    "# OSM data is concat'd [inters vs non-inters], and then the intersection features are max-aggregated\n",
    "# Keep_feat controls which OSM features to keep\n",
    "aggregated, OSM_feat = aggregate(PROCESSED_DIR=PROCESSED_DIR)\n",
    "\n",
    "aggregated['correlate_id'] = aggregated.index\n",
    "aggregated.to_csv(os.path.join(PROCESSED_DIR, 'roads.csv.gz'), compression='gzip')\n",
    "aggregated_geojson = df_to_geojson(aggregated, properties=['display_name'])\n",
    "with open(os.path.join(MAP_FP, 'aggregated.geojson'), 'w') as outfile:\n",
    "    geojson.dump(aggregated_geojson, outfile)\n",
    "    \n",
    "crash_OSM_merged = pd.merge(crash, aggregated, left_on='near_id', right_on='correlate_id', how='left')\n",
    "print(\"Exporting crash dataset to \", PROCESSED_DIR)\n",
    "crash_OSM_merged.to_csv(os.path.join(PROCESSED_DIR, 'crash.csv.gz'), compression='gzip')\n",
    "crash_OSM_merged.to_json(os.path.join(PROCESSED_DIR, 'crashes_min.json'), orient=\"records\")\n",
    "print(\"Number of unique road segments: {} Number of unique nearIDs in crashes: {}\".format(len(aggregated), crash_OSM_merged.nunique().near_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set target to 1 for positive example\n",
    "crash_OSM_merged['TARGET'] = 1\n",
    "aggregated.to_csv(os.path.join(PROCESSED_DIR, 'roads.csv.gz'), compression='gzip')\n",
    "with open(os.path.join(PROCESSED_DIR, 'roads.pk'), 'wb') as fp:\n",
    "    pickle.dump(aggregated, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1953,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1995,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1958,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_open = pd.read_csv(os.path.join(PROCESSED_DIR, 'roads.csv.gz'), dtype={'correlate_id': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1959,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4961"
      ]
     },
     "execution_count": 1959,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_open.correlate_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make negative data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_data = make_negative_data(crash=crash_OSM_merged, roads=aggregated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any columns within positive data that don't occur in negative data\n",
    "neg_cols = list(negative_data)\n",
    "positive_data = crash_OSM_merged[neg_cols]\n",
    "\n",
    "# Negative and positive data is concatenated\n",
    "# We make accident_number the index so that we index properly in add_accident_count\n",
    "# We don't drop the accident_no column however, as we also refer to this in add_accident_count\n",
    "data_set = pd.concat([positive_data, negative_data]).set_index('ACCIDENT_NO', drop=False)\n",
    "\n",
    "# Data set has accident counts attached to it\n",
    "data_set_with_accident_count = add_accident_count(data_set, PROCESSED_DIR)\n",
    "\n",
    "# Output canonical dataset\n",
    "print(\"Exporting canonical dataset to \", PROCESSED_DIR)\n",
    "data_set_with_accident_count.to_csv(os.path.join(PROCESSED_DIR, 'canon.csv.gz'), compression='gzip')\n",
    "\n",
    "data_set_with_accident_count.loc[data_set_with_accident_count['correlate_id']=='002']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.join(BASE_DIR, 'src', 'models'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_utils import format_crash_data\n",
    "from model_classes import Indata, Tuner, Tester\n",
    "import sklearn.linear_model as skl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as ss\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import yaml\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(BASE_DIR, 'data', config['name'])\n",
    "PROCESSED_DATA_DIR = os.path.join(BASE_DIR, 'data', config['name'], 'processed/')\n",
    "merged_data_path = os.path.join(PROCESSED_DATA_DIR, config['merged_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(config, data, datadir):\n",
    "    \"\"\"\n",
    "    Get features from the feature list created during data generation / or the list specified during init_city.\n",
    "    \"\"\"\n",
    "\n",
    "    cont_feat = config['cont_feat']\n",
    "    cat_feat = config['cat_feat']\n",
    "\n",
    "    # Dropping continuous features that don't exist\n",
    "    cont_feat_found = []\n",
    "    for f in cont_feat:\n",
    "        if f not in data.columns.values:\n",
    "            print(\"Feature \" + f + \" not found, skipping\")\n",
    "        else:\n",
    "            cont_feat_found.append(f)\n",
    "\n",
    "    # Dropping categorical features that don't exist\n",
    "    cat_feat_found = []\n",
    "    for f in cat_feat:\n",
    "        if f not in data.columns.values:\n",
    "            print(\"Feature \" + f + \" not found, skipping\")\n",
    "        else:\n",
    "            cat_feat_found.append(f)\n",
    "\n",
    "    # Create featureset holder\n",
    "    features = cont_feat_found + cat_feat_found\n",
    "\n",
    "    return cont_feat_found, cat_feat_found, features\n",
    "\n",
    "\n",
    "def process_features(data, features, config, f_cat, f_cont):\n",
    "\n",
    "    print('Within train_model.process_features')\n",
    "\n",
    "    # Features for linear model\n",
    "    linear_model_features = features\n",
    "\n",
    "    # Turn categorical variables into one-hot representation through get_dummies\n",
    "    # Append the newly named one-hot variables [e.g. hwy_type23] to our data frame\n",
    "    # Append the new feature names to our feature list\n",
    "    # For linear model features leave out the first one [e.g. hwy_type0 for intercept (?)]\n",
    "    print('Processing categorical variables [one-hot encoding]')\n",
    "    for f in f_cat:\n",
    "        temp = pd.get_dummies(data[f])\n",
    "        temp.columns = [f + '_' + str(c) for c in temp.columns]\n",
    "        data = pd.concat([data, temp], axis=1)\n",
    "        features += temp.columns.tolist()\n",
    "        linear_model_features += temp.columns.tolist()[1:]\n",
    "\n",
    "    # Turn continuous variables into their log [add one to avoid -inf errors]\n",
    "    # Using 1.0 rather than 1 to typecast as float rather than int. This is required for log transfrom.\n",
    "    # Append new feature name to relevant lists[e.g. log_width]\n",
    "    print('Processing continuous variables [log-transform]')\n",
    "    for f in f_cont:\n",
    "        data['log_%s' % f] = np.log(data[f] + 1.0)\n",
    "        features += ['log_%s' % f]\n",
    "        linear_model_features += ['log_%s' % f]\n",
    "\n",
    "    # Remove duplicated features\n",
    "    # e.g. if features = ['a', 'b', 'c', 'b'], f_cat = ['a'] and f_cont=['b']\n",
    "    # then set(features) = {'a', 'b', 'c'} and set(f_cat + f_cont) = {'a', 'b'}\n",
    "\n",
    "    features = list(set(features) - set(f_cat + f_cont))\n",
    "    linear_model_features = list(set(linear_model_features) - set(f_cat + f_cont))\n",
    "\n",
    "    return data, features, linear_model_features\n",
    "\n",
    "\n",
    "def output_importance(trained_model, features, datadir):\n",
    "    # output feature importances or coefficients\n",
    "    if hasattr(trained_model, 'feature_importances_'):\n",
    "        feature_imp_dict = dict(zip(features, trained_model.feature_importances_.astype(float)))\n",
    "    elif hasattr(trained_model, 'coefficients'):\n",
    "        feature_imp_dict = dict(zip(features, trained_model.coefficients.astype(float)))\n",
    "    else:\n",
    "        return(\"No feature importances/coefficients detected\")\n",
    "    # conversion to json\n",
    "    with open(os.path.join(datadir, 'feature_importances.json'), 'w') as f:\n",
    "        json.dump(feature_imp_dict, f)\n",
    "\n",
    "\n",
    "def set_params():\n",
    "\n",
    "    # cv parameters\n",
    "    cvp = dict()\n",
    "    cvp['pmetric'] = 'roc_auc'\n",
    "    cvp['iter'] = 5  # number of iterations\n",
    "    cvp['folds'] = 5  # folds for cv (default)\n",
    "    cvp['shuffle'] = True\n",
    "\n",
    "    # LR parameters\n",
    "    mp = dict()\n",
    "    mp['LogisticRegression'] = dict()\n",
    "    mp['LogisticRegression']['penalty'] = ['l2']\n",
    "    mp['LogisticRegression']['C'] = ss.beta(a=5, b=2)  # beta distribution for selecting reg strength\n",
    "    mp['LogisticRegression']['class_weight'] = ['balanced']\n",
    "    mp['LogisticRegression']['solver'] = ['lbfgs']\n",
    "\n",
    "    # xgBoost model parameters\n",
    "    mp['XGBClassifier'] = dict()\n",
    "    mp['XGBClassifier']['max_depth'] = list(range(3, 7))\n",
    "    mp['XGBClassifier']['min_child_weight'] = list(range(1, 5))\n",
    "    mp['XGBClassifier']['learning_rate'] = ss.beta(a=2, b=15)\n",
    "\n",
    "    # cut-off for model performance\n",
    "    # generally, if the model isn't better than chance, it's not worth reporting\n",
    "    perf_cutoff = 0.5\n",
    "    return cvp, mp, perf_cutoff\n",
    "\n",
    "\n",
    "def initialize_and_run(data_model, features, linear_model_features, datadir, target, seed=None):\n",
    "\n",
    "    print('Within train_model.initialize_and_run')\n",
    "    print('Will now set initial model parameters, created our InData object, split into train / test sets')\n",
    "\n",
    "    # Cross-validation parameters, model parameters, perf_cutoff\n",
    "    cvp, mp, perf_cutoff = set_params()\n",
    "\n",
    "    # Initialize data with __init__ method\n",
    "    # Parameters (self, data, target, scoring=None\n",
    "    # Returns object with properties: .data, .target, .scoring [if provided]\n",
    "    # With attributes: scoring, data, train_x, train_y, test_x, test_y, is_split\n",
    "    df = Indata(data_model, target)\n",
    "\n",
    "    # Create train/test split\n",
    "    # Parameters (self, pct, datesort=None, group_col=None, seed=None)\n",
    "    df.tr_te_split(.7, seed=seed)\n",
    "\n",
    "    # Create weighting variable and attach to parameters\n",
    "    # This is intended to weight data if it is imbalanced.\n",
    "    # a[0] = frequency of negative class, a[1] = frequency of positive class\n",
    "    # normalize = True means .value_counts returns relative frequencies, not absolute count\n",
    "    a = data_model[target].value_counts(normalize=True)\n",
    "    w = 1 / a[1]\n",
    "    mp['XGBClassifier']['scale_pos_weight'] = [w]\n",
    "\n",
    "    # Initialize tuner\n",
    "    # Tuner takes the attributes [self, indata, best_models=None, grid_results=None)]\n",
    "    print('Having done our base initialisation, we attempt to tune model using tuner object')\n",
    "    tune = Tuner(df)\n",
    "    try:\n",
    "        # Base XGBoost model and then base Logistic Regression model\n",
    "        # Tuning method has the parameters [self, name, m_name, features, cvparams, mparams]\n",
    "        tune.tune('XG_base', 'XGBClassifier', features, cvp, mp['XGBClassifier'])\n",
    "        tune.tune('LR_base', 'LogisticRegression', linear_model_features, cvp, mp['LogisticRegression'])\n",
    "\n",
    "    except ValueError:\n",
    "        print('CV fails, likely very few of target available, try rerunning at segment-level')\n",
    "        raise\n",
    "\n",
    "    # Initialise and run tester object to find best performing model\n",
    "    print('Tuning finished, running against test data')\n",
    "    test = Tester(df)\n",
    "    test.init_tuned(tune)\n",
    "    test.run_tuned('LR_base', cal=False)\n",
    "    test.run_tuned('XG_base', cal=False)\n",
    "\n",
    "    # choose best performing model\n",
    "    print('Within train_model. Have instantiated tuner object and completed tuning. Will now iterate over test.rundict to check for best performing model. Test.rundict has len:', len(test.rundict), 'and looks like:', test.rundict)\n",
    "    best_perf = 0\n",
    "    best_model = None\n",
    "    for m in test.rundict:\n",
    "        if test.rundict[m]['roc_auc'] > best_perf:\n",
    "            best_perf = test.rundict[m]['roc_auc']\n",
    "            best_model = test.rundict[m]['model']\n",
    "            best_model_features = test.rundict[m]['features']\n",
    "\n",
    "    # Check for performance above certain level\n",
    "    if best_perf <= perf_cutoff:\n",
    "        print(('Model performs below AUC %s, may not be usable' % perf_cutoff))\n",
    "\n",
    "    # Train on full data\n",
    "    print('Best performance was', best_perf, '\\n Best model was', best_model, '\\nBest model features were', best_model_features)\n",
    "    trained_model = best_model.fit(data_model[best_model_features], data_model[target])\n",
    "\n",
    "    # Output feature importance\n",
    "    output_importance(trained_model, features, datadir)\n",
    "\n",
    "    # Save out best model to pickle for later use\n",
    "    with open(os.path.join(datadir, 'model.pk'), 'wb') as fp:\n",
    "        pickle.dump(trained_model, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CONFIG_FP) as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "data = pd.read_csv(merged_data_path)\n",
    "data.sort_values(['DATE_TIME'], inplace=True)\n",
    "\n",
    "# Get all features that exist within dataset and are being used\n",
    "f_cont, f_cat, features = get_features(config, data, PROCESSED_DATA_DIR)\n",
    "print('Our categorical features are:', f_cat)\n",
    "print('Our continuous features are:', f_cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove features that aren't part of f_cat or f_cont or TARGET\n",
    "data_model = data[f_cat + f_cont + ['TARGET']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_model, features, linear_model_features = process_features(data_model, features, config, f_cat, f_cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out various statistics to understand model parameters\n",
    "print(\"full features:{}\".format(features))\n",
    "print('\\n\\n Data_model: \\n\\n', data_model)\n",
    "print('\\n\\n features:', features)\n",
    "print('\\n\\n lm_features:', linear_model_features)\n",
    "print('\\n\\n Process_DATA_DIR:', PROCESSED_DATA_DIR)\n",
    "\n",
    "# Save out data_model and the features within\n",
    "data_model_path = os.path.join(PROCESSED_DATA_DIR, 'myDataModel.csv')\n",
    "if not os.path.exists(data_model_path) or True:\n",
    "    data_model.to_csv(data_model_path, index=False)\n",
    "\n",
    "features_path = data_model_path = os.path.join(PROCESSED_DATA_DIR, 'features.pk')\n",
    "if not os.path.exists(features_path) or True:\n",
    "    with open(features_path, 'wb') as fp:\n",
    "        pickle.dump(features, fp)\n",
    "\n",
    "initialize_and_run(data_model, features, linear_model_features, PROCESSED_DATA_DIR, target='TARGET')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_utils import format_crash_data\n",
    "from model_classes import Indata, Tuner, Tester\n",
    "from train_model import process_features, get_features\n",
    "import sklearn.linear_model as skl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1989,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(trained_model, predict_data, features, data_model_features, DATA_DIR):\n",
    "    \"\"\"\n",
    "    Returns\n",
    "        nothing, writes prediction segments to file\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure predict_data has the same columns and column ordering as required by trained_model\n",
    "    predict_data_reduced = predict_data[data_model_features]\n",
    "    preds = trained_model.predict_proba(predict_data_reduced)[::, 1]\n",
    "    predict_data['predictions'] = preds\n",
    "\n",
    "    predict_data.to_csv(os.path.join(DATA_DIR, 'predictions.csv'), index=False)\n",
    "    predict_data.to_json(os.path.join(DATA_DIR, 'predictions.json'), orient='index')\n",
    "    with open(os.path.join(DATA_DIR, 'predictions.pk'), 'wb') as fp:\n",
    "        pickle.dump(predict_data, fp)\n",
    "    \n",
    "    print(type(predict_data.coordinates[0]))\n",
    "\n",
    "\n",
    "def get_accident_count_recent(predict_data, data):\n",
    "    data['DATE_TIME'] = pd.to_datetime(data['DATE_TIME'])\n",
    "\n",
    "    current_date = datetime.now()\n",
    "    past_7_days = current_date - pd.to_timedelta(\"7day\")\n",
    "    past_30_days = current_date - pd.to_timedelta(\"30day\")\n",
    "    past_365_days = current_date - pd.to_timedelta(\"365day\")\n",
    "    past_1825_days = current_date - pd.to_timedelta(\"1825day\")\n",
    "    past_3650_days = current_date - pd.to_timedelta(\"3650day\")\n",
    "\n",
    "    recent_crash_7 = data.loc[data['DATE_TIME'] > past_7_days]\n",
    "    recent_crash_30 = data.loc[data['DATE_TIME'] > past_30_days]\n",
    "    recent_crash_365 = data.loc[data['DATE_TIME'] > past_365_days]\n",
    "    recent_crash_1825 = data.loc[data['DATE_TIME'] > past_1825_days]\n",
    "    recent_crash_3650 = data.loc[data['DATE_TIME'] > past_3650_days]\n",
    "\n",
    "    column_names = ['LAST_7_DAYS', 'LAST_30_DAYS', 'LAST_365_DAYS', 'LAST_1825_DAYS', 'LAST_3650_DAYS']\n",
    "    recent_crashes = [recent_crash_7, recent_crash_30, recent_crash_365, recent_crash_1825, recent_crash_3650]\n",
    "\n",
    "    for col_name in column_names:\n",
    "        predict_data[col_name] = \"\"\n",
    "\n",
    "    i = 0\n",
    "    print('About to append recent accident counts. This will take some time.')\n",
    "    for i in range(len(predict_data)):\n",
    "        current_segment_id = predict_data.loc[i].segment_id\n",
    "\n",
    "        for j in range(len(recent_crashes)):\n",
    "\n",
    "            # Find number of crashes at same segment that have occured in appropriate time period\n",
    "            recent_crash = recent_crashes[j]\n",
    "            num_crashes = len(recent_crash.loc[recent_crash['segment_id'] == current_segment_id])\n",
    "\n",
    "            # Assign this number to predict_data\n",
    "            col_name = column_names[j]\n",
    "            predict_data.at[i, col_name] = num_crashes\n",
    "\n",
    "        if i % 5000 == 0:\n",
    "            print(\"Got through {}% of results\".format(100 * i / len(predict_data)))\n",
    "\n",
    "    return predict_data\n",
    "\n",
    "\n",
    "def add_empty_features(predict_data, features):\n",
    "\n",
    "    # Read in the features from our modelling dataset\n",
    "    features_path = os.path.join(PROCESSED_DATA_DIR, 'features.pk')\n",
    "    with open(features_path, 'rb') as fp:\n",
    "        data_model_features = pickle.load(fp)\n",
    "\n",
    "    # Get the difference of features between our modelling dataset and predicting dataset\n",
    "    # Recast as a list to allow for looping over\n",
    "    feature_difference = list(set(data_model_features) - set(features))\n",
    "\n",
    "    # Add features in a loop as python doens't like adding all at one time\n",
    "    for feat in feature_difference:\n",
    "        predict_data[feat] = 0\n",
    "\n",
    "    return predict_data, feature_difference, data_model_features\n",
    "\n",
    "def process_features(data, features, config, f_cat, f_cont):\n",
    "\n",
    "    print('Within train_model.process_features')\n",
    "\n",
    "    # Features for linear model\n",
    "    linear_model_features = features\n",
    "\n",
    "    # Turn categorical variables into one-hot representation through get_dummies\n",
    "    # Append the newly named one-hot variables [e.g. hwy_type23] to our data frame\n",
    "    # Append the new feature names to our feature list\n",
    "    # For linear model features leave out the first one [e.g. hwy_type0 for intercept (?)]\n",
    "    print('Processing categorical variables [one-hot encoding]')\n",
    "    for f in f_cat:\n",
    "        temp = pd.get_dummies(data[f])\n",
    "        temp.columns = [f + '_' + str(c) for c in temp.columns]\n",
    "        data = pd.concat([data, temp], axis=1)\n",
    "        features += temp.columns.tolist()\n",
    "        linear_model_features += temp.columns.tolist()[1:]\n",
    "\n",
    "    # Turn continuous variables into their log [add one to avoid -inf errors]\n",
    "    # Using 1.0 rather than 1 to typecast as float rather than int. This is required for log transform [base e].\n",
    "    # Also requires a temp array to hold the values with a recasting, otherwise numpy tries to do things like 5.log, rather than log(5)\n",
    "    # Append new feature name to relevant lists[e.g. log_width]\n",
    "    print('Processing continuous variables [log-transform]')\n",
    "    for f in f_cont:\n",
    "        temp_array = np.array((data[f] + 1).values).astype(np.float64)\n",
    "        data['log_%s' % f] = np.log(temp_array)\n",
    "        features += ['log_%s' % f]\n",
    "        linear_model_features += ['log_%s' % f]\n",
    "\n",
    "    # Remove duplicated features\n",
    "    # e.g. if features = ['a', 'b', 'c', 'b'], f_cat = ['a'] and f_cont=['b']\n",
    "    # then set(features) = {'a', 'b', 'c'} and set(f_cat + f_cont) = {'a', 'b'}\n",
    "\n",
    "    features = list(set(features) - set(f_cat + f_cont))\n",
    "    linear_model_features = list(set(linear_model_features) - set(f_cat + f_cont))\n",
    "\n",
    "    return data, features, linear_model_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1994,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Daniel\\\\Documents\\\\ML\\\\Transurban V2\\\\data\\\\Melbourne\\\\processed/roads.csv.gz'"
      ]
     },
     "execution_count": 1994,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "road_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2005,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in road data. We shall generate a prediction for each segment.\n",
    "#predict_data = pd.read_csv(road_data_path, dtype={'correlate_id': str})\n",
    "with open(os.path.join(PROCESSED_DIR, 'roads.pk'), 'rb') as fp:\n",
    "    predict_data = pickle.load(fp)\n",
    "\n",
    "# Reset the index so that it can be poperly looped over in the attach accident count phase\n",
    "predict_data.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "# Read in crash data. We shall use this to attach historic accident counts to road data.\n",
    "data = pd.read_csv(crash_data_path, dtype={'near_id': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2006,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing set() columns from predict data due to NA\n",
      "Removing set() columns from crash data due to NA\n"
     ]
    }
   ],
   "source": [
    "# Check NA within both DF\n",
    "predict_na = (predict_data.isna().sum())/len(predict_data)\n",
    "data_na = (data.isna().sum())/len(data)\n",
    "\n",
    "data_cols = (data_na < 0.95).keys()\n",
    "predict_cols = (predict_na < 0.95).keys()\n",
    "\n",
    "print('Removing {} columns from predict data due to NA'.format(set(list(predict_data)) - set(predict_cols)))\n",
    "print('Removing {} columns from crash data due to NA'.format(set(list(data)) - set(data_cols)))\n",
    "\n",
    "predict_data = predict_data[predict_cols]\n",
    "data = data[data_cols]\n",
    "\n",
    "predict_data.fillna('', inplace=True)\n",
    "data.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2007,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to append recent accident counts. This will take some time.\n",
      "Got through 0.0% of results\n",
      "Feature dead_end not found, skipping\n",
      "Feature streets not found, skipping\n",
      "Within train_model.process_features\n",
      "Processing categorical variables [one-hot encoding]\n",
      "Processing continuous variables [log-transform]\n"
     ]
    }
   ],
   "source": [
    "# Attach current date / time data\n",
    "date_time = datetime.now()\n",
    "hour = date_time.hour\n",
    "day = date_time.weekday()\n",
    "month = date_time.month\n",
    "\n",
    "predict_data['MONTH'] = month\n",
    "predict_data['DAY_OF_WEEK'] = day\n",
    "predict_data['HOUR'] = hour\n",
    "\n",
    "# Attach accident data\n",
    "predict_path = os.path.join(PROCESSED_DIR, 'predict.csv.gz')\n",
    "if not os.path.exists(predict_path) or True:\n",
    "    predict_data = get_accident_count_recent(predict_data, data)\n",
    "    predict_data.to_csv(predict_path, index=False, compression='gzip')\n",
    "else:\n",
    "    predict_data = pd.read_csv(predict_path, dtype={'correlate_id': str})\n",
    "    \n",
    "f_cont, f_cat, features = get_features(config, predict_data, PROCESSED_DATA_DIR)\n",
    "predict_data, features, _ = process_features(predict_data, features, config, f_cat, f_cont)\n",
    "predict_data, added_features, data_model_features = add_empty_features(predict_data, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2010,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Read in best performing model from train_model\n",
    "with open(os.path.join(PROCESSED_DATA_DIR, 'model.pk'), 'rb') as fp:\n",
    "    trained_model = pickle.load(fp)\n",
    "\n",
    "# Get predictions from model and prediction features\n",
    "predict(trained_model=trained_model, predict_data=predict_data, features=features, data_model_features=data_model_features, DATA_DIR=DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising road geometries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2018,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2030,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_geojson = df_to_geojson(predictions, properties=properties)\n",
    "predictions_fp = os.path.join(PROCESSED_DATA_DIR, 'preds_final.geojson')\n",
    "predictions_fp_2 = os.path.join(BASE_DIR, 'reports', 'Melbourne', 'preds.txt')\n",
    "with open(predictions_fp, \"w\") as fp:\n",
    "    geojson.dump(predictions_geojson, fp)\n",
    "with open(predictions_fp_2, \"w\") as fp:\n",
    "    geojson.dump(predictions_geojson, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2028,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2029,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_file = os.path.join(MAP_DIR, \"inter_and_non_int.geojson\")\n",
    "if not os.path.exists(segments_file):\n",
    "    sys.exit(\"segment file not found at {}, exiting\".format(segments_file))\n",
    "else:\n",
    "    segments = pd.read_json(segments_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Within make_preds_viz. About to turn dataframe into geojson. This takes a moderate amount of time [2-3 minutes].')\n",
    "predictions_geojson = df_to_geojson(df=predictions_merged, properties=keep_cols)\n",
    "\n",
    "# Output as geojson\n",
    "predictions_collection = geojson.FeatureCollection(predictions_geojson)\n",
    "predictions_fp = os.path.join(PROCESSED_DATA_DIR, 'preds_final.geojson')\n",
    "with open(predictions_fp, \"w\") as fp:\n",
    "    geojson.dump(predictions_collection, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import geojson\n",
    "import sys\n",
    "import yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def df_to_geojson(df, properties):\n",
    "    \"\"\"\n",
    "    Turn a dataframe containing point data into a geojson formatted python dictionary\n",
    "\n",
    "    df : the dataframe to convert to geojson\n",
    "    properties : a list of columns in the dataframe to turn into geojson feature properties\n",
    "    lat : the name of the column in the dataframe that contains latitude data\n",
    "    lon : the name of the column in the dataframe that contains longitude data\n",
    "    \"\"\"\n",
    "\n",
    "    # create a new python dict to contain our geojson data, using geojson format\n",
    "    geojson = {'type': 'FeatureCollection', 'features': []}\n",
    "\n",
    "    # Create a secondary properties list which doesn't include type or coordinates\n",
    "    properties_update = []\n",
    "    for prop in properties:\n",
    "        if prop not in ['type', 'coordinates', 'geometry.type']:\n",
    "            properties_update.append(prop)\n",
    "\n",
    "    # loop through each row in the dataframe and convert each row to geojson format\n",
    "    for _, row in df.iterrows():\n",
    "        # create a feature template to fill in\n",
    "        feature = {'type': row['type'],\n",
    "                   'properties': {},\n",
    "                   'geometry': {'type': row['geometry.type'],\n",
    "                                'coordinates': row['coordinates']}\n",
    "                   }\n",
    "\n",
    "        # For each column, get the value and add it as a new feature property\n",
    "        for prop in properties_update:\n",
    "            feature['properties'][prop] = row[prop]\n",
    "\n",
    "        # add this feature (aka, converted dataframe row) to the list of features inside our dict\n",
    "        geojson['features'].append(feature)\n",
    "\n",
    "    return geojson\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Confirm files exist & load data\n",
    "predictions_file = os.path.join(DATA_DIR, \"predictions.json\")\n",
    "if not os.path.exists(predictions_file):\n",
    "    sys.exit(\"Predictions file not found at {}, exiting\".format(predictions_file))\n",
    "\n",
    "# Load the predictions\n",
    "predictions = pd.read_json(predictions_file, orient='index')\n",
    "print(\"Loaded predictions. Found {} datapoints\".format(len(predictions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the road segments file\n",
    "segments_file = os.path.join(MAP_DIR, \"inter_and_non_int.geojson\")\n",
    "if not os.path.exists(segments_file):\n",
    "    sys.exit(\"segment file not found at {}, exiting\".format(segments_file))\n",
    "else:\n",
    "    segments = pd.read_json(segments_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Post process segments. In particular, json_normalize flattens out the nested json string in segments['features']\n",
    "segments = segments[\"features\"]\n",
    "segments = pd.io.json.json_normalize(segments, record_prefix=False)\n",
    "print('Loaded segments, found {} results'.format(len(segments)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "segments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Post process predictions. In particular, drop center_x and center_y as this data is in segments, and it looks like the predictions data is buggy here\n",
    "predictions.drop(['coordinates', 'type', 'geometry.type'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments.id = segments.id.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two. This gives the necessary geoJSON features to our predictions DF [though they may already be there]\n",
    "predictions_merged = pd.merge(predictions, segments, left_on='correlate_id', right_on='id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.loc[predictions['correlate_id'] == '000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Rename relevant columns\n",
    "predictions_merged.rename(columns={'geometry.coordinates': 'coordinates', 'properties.center_x': 'center_x', 'properties.center_y': 'center_y'}, inplace=True)\n",
    "\n",
    "# Get rid of unwanted columns and rename those that have gained prefixes during flattening.\n",
    "keep_cols = config['cont_feat'] + config['cat_feat'] + ['segment_id', 'coordinates', 'type', 'geometry.type', 'center_x', 'center_y', 'predictions']\n",
    "predictions_merged = predictions_merged[keep_cols]\n",
    "\n",
    "# Get rid of some NA values that occur in 'display_name' and 'node_type_int'\n",
    "previous_len = len(predictions)\n",
    "predictions['display_name'].fillna(\"No name found.\", inplace=True)\n",
    "if pd.isnull(predictions).sum().sum() > 0:\n",
    "    print('Within make_preds_viz. Some NA values still found within dataframe. Exiting')\n",
    "    sys.exit()\n",
    "print('Filled {} out of a total of {} results due to NA values'.format(len(predictions) - previous_len, len(predictions)))\n",
    "\n",
    "# Turn back into geojson to write out\n",
    "print('Within make_preds_viz. About to turn dataframe into geojson. This takes a moderate amount of time [2-3 minutes].')\n",
    "predictions_geojson = df_to_geojson(df=predictions_merged, properties=keep_cols)\n",
    "\n",
    "# Output as geojson\n",
    "predictions_collection = geojson.FeatureCollection(predictions_geojson)\n",
    "predictions_fp = os.path.join(PROCESSED_DATA_DIR, 'preds_final.geojson')\n",
    "with open(predictions_fp, \"w\") as fp:\n",
    "    geojson.dump(predictions_collection, fp)\n",
    "\n",
    "print(\"Wrote {} assembled predictions to file {}\".format(len(predictions_collection['features']), predictions_fp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make GEOJSON of roads for visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = predictions_merged\n",
    "properties = keep_cols\n",
    "\n",
    "geojson = {'type': 'FeatureCollection', 'features': []}\n",
    "\n",
    "# Create a secondary properties list which doesn't include type or coordinates\n",
    "properties_update = []\n",
    "for prop in properties:\n",
    "    if prop not in ['type', 'coordinates', 'geometry.type']:\n",
    "        properties_update.append(prop)\n",
    "\n",
    "# Loop through each row in the dataframe and convert each row to geojson format\n",
    "for _, row in df.iterrows():\n",
    "    # create a feature template to fill in\n",
    "    feature = {'type': row['type'],\n",
    "               'properties': {},\n",
    "               'geometry': {'type': row['geometry.type'],\n",
    "                            'coordinates': row['coordinates']}\n",
    "               }\n",
    "\n",
    "    # For each column, get the value and add it as a new feature property\n",
    "    for prop in properties_update:\n",
    "        feature['properties'][prop] = row[prop]\n",
    "\n",
    "    # Add this feature (aka, converted dataframe row) to the list of features inside our dict\n",
    "    geojson['features'].append(feature)\n",
    "\n",
    "predictions_geojson = geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn back into geojson to write out\n",
    "print('Within make_preds_viz. About to turn dataframe into geojson. This takes a moderate amount of time [2-3 minutes].')\n",
    "predictions_geojson = df_to_geojson(df=predictions_merged, properties=keep_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output as geojson\n",
    "predictions_collection = geojson.FeatureCollection(predictions_geojson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_fp = os.path.join(PROCESSED_DATA_DIR, 'preds_final.geojson')\n",
    "with open(predictions_fp, \"w\") as fp:\n",
    "    geojson.dump(predictions_geojson, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for road in predictions_geojson['features']:\n",
    "    if road['properties']['segment_id'] == '39613129-387153039-2207661111':\n",
    "        print(road)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_df = pd.read_csv(\"../data/Melbourne/processed/crash.csv.gz\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_df.to_json(orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_df.to_json('../data/Melbourne/processed/crashes_min_records.json', orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_df.to_json(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
